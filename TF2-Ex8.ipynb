{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN and WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Libs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dev20200315\n"
     ]
    }
   ],
   "source": [
    "# All required libs are imporeted in this cell\n",
    "import os, glob, multiprocessing\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, ReLU, LeakyReLU, Activation\n",
    "from tensorflow.keras import Sequential, optimizers, Input, layers\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "z_dim = 100\n",
    "epochs = 3000000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "is_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Functions/Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Files: 51223\n",
      "<PrefetchDataset shapes: (128, 64, 64, 3), types: tf.float32> (64, 64, 3)\n",
      "(128, 64, 64, 3) 1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n",
    "\n",
    "    # @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.image.resize(img, [resize, resize])\n",
    "        # img = tf.image.random_crop(img,[resize, resize])\n",
    "        # img = tf.image.random_flip_left_right(img)\n",
    "        # img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.clip_by_value(img, 0, 255)\n",
    "        img = img / 127.5 - 1 #-1~1\n",
    "        return img\n",
    "\n",
    "    dataset = disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) // batch_size\n",
    "\n",
    "    return dataset, img_shape, len_dataset\n",
    "\n",
    "\n",
    "def batch_dataset(dataset,\n",
    "                  batch_size,\n",
    "                  drop_remainder=True,\n",
    "                  n_prefetch_batch=1,\n",
    "                  filter_fn=None,\n",
    "                  map_fn=None,\n",
    "                  n_map_threads=None,\n",
    "                  filter_after_map=False,\n",
    "                  shuffle=True,\n",
    "                  shuffle_buffer_size=None,\n",
    "                  repeat=None):\n",
    "    # set defaults\n",
    "    if n_map_threads is None:\n",
    "        n_map_threads = multiprocessing.cpu_count()\n",
    "    if shuffle and shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
    "\n",
    "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if not filter_after_map:\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "    else:  # [*] this is slower\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def memory_data_batch_dataset(memory_data,\n",
    "                              batch_size,\n",
    "                              drop_remainder=True,\n",
    "                              n_prefetch_batch=1,\n",
    "                              filter_fn=None,\n",
    "                              map_fn=None,\n",
    "                              n_map_threads=None,\n",
    "                              filter_after_map=False,\n",
    "                              shuffle=True,\n",
    "                              shuffle_buffer_size=None,\n",
    "                              repeat=None):\n",
    "    \"\"\"Batch dataset of memory data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_data : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
    "    dataset = batch_dataset(dataset,\n",
    "                            batch_size,\n",
    "                            drop_remainder=drop_remainder,\n",
    "                            n_prefetch_batch=n_prefetch_batch,\n",
    "                            filter_fn=filter_fn,\n",
    "                            map_fn=map_fn,\n",
    "                            n_map_threads=n_map_threads,\n",
    "                            filter_after_map=filter_after_map,\n",
    "                            shuffle=shuffle,\n",
    "                            shuffle_buffer_size=shuffle_buffer_size,\n",
    "                            repeat=repeat)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def disk_image_batch_dataset(img_paths,\n",
    "                             batch_size,\n",
    "                             labels=None,\n",
    "                             drop_remainder=True,\n",
    "                             n_prefetch_batch=1,\n",
    "                             filter_fn=None,\n",
    "                             map_fn=None,\n",
    "                             n_map_threads=None,\n",
    "                             filter_after_map=False,\n",
    "                             shuffle=True,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             repeat=None):\n",
    "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_paths : 1d-tensor/ndarray/list of str\n",
    "        labels : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        memory_data = img_paths\n",
    "    else:\n",
    "        memory_data = (img_paths, labels)\n",
    "\n",
    "    def parse_fn(path, *label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n",
    "        return (img,) + label\n",
    "\n",
    "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
    "        def map_fn_(*args):\n",
    "            return map_fn(*parse_fn(*args))\n",
    "    else:\n",
    "        map_fn_ = parse_fn\n",
    "\n",
    "    dataset = memory_data_batch_dataset(memory_data,\n",
    "                                        batch_size,\n",
    "                                        drop_remainder=drop_remainder,\n",
    "                                        n_prefetch_batch=n_prefetch_batch,\n",
    "                                        filter_fn=filter_fn,\n",
    "                                        map_fn=map_fn_,\n",
    "                                        n_map_threads=n_map_threads,\n",
    "                                        filter_after_map=filter_after_map,\n",
    "                                        shuffle=shuffle,\n",
    "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                        repeat=repeat)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_random_z(z_dim, batch_size):\n",
    "    return tf.random.uniform([batch_size, z_dim], minval=-1, maxval=1)\n",
    "\n",
    "\n",
    " \n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    #toimage(final_image).save(image_path)\n",
    "    #print(image_path)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "img_path = glob.glob('./faces/*.jpg')\n",
    "print(\"Num of Files:\",len(img_path))\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize = 64)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset))\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(), tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat(100)\n",
    "db_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions and Test Code\n",
    "### SubClassing Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN can re-use DCGAN with minor modifications:\n",
    "# No BatchNormalization, No sigmoid output for discriminator\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.s = 2\n",
    "        self.k = 4\n",
    "        self.n_f = 1024\n",
    "        \n",
    "        self.dense1 = Dense(self.s * self.s * self.n_f)\n",
    "        self.reshape1 = Reshape(target_shape = (self.s, self.s, self.n_f))\n",
    "        self.conv1 = Conv2DTranspose(512, self.k, 2, 'same')\n",
    "        self.conv2 = Conv2DTranspose(256, self.k, 2, 'same')\n",
    "        self.conv3 = Conv2DTranspose(128, self.k, 2, 'same')\n",
    "        self.conv4 = Conv2DTranspose(64, self.k, 2, 'same')\n",
    "        self.conv5 = Conv2DTranspose(3, self.k, 2, 'same')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.reshape1(x)\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.conv1(x))\n",
    "        x = tf.nn.leaky_relu(self.conv2(x))\n",
    "        x = tf.nn.leaky_relu(self.conv3(x))\n",
    "        x = tf.nn.leaky_relu(self.conv4(x))\n",
    "        x = tf.tanh(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.s = 4\n",
    "        self.k = 4\n",
    "        \n",
    "        self.conv1 = Conv2D(64, self.k, 2, 'same')\n",
    "        self.conv2 = Conv2D(128, self.k, 2, 'same')\n",
    "        self.conv3 = Conv2D(256, self.k, 2, 'same')\n",
    "        self.conv4 = Conv2D(512, self.k, 2, 'same')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs\n",
    "        x = tf.nn.leaky_relu(self.conv1(x))\n",
    "        x = tf.nn.leaky_relu(self.conv2(x))\n",
    "        x = tf.nn.leaky_relu(self.conv3(x))\n",
    "        x = tf.nn.leaky_relu(self.conv4(x))\n",
    "        x = self.dense(self.flatten(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Generator_s and Discriminator_s are lite versions of DCGAN\n",
    "class Generator_s(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator_s, self).__init__()\n",
    "\n",
    "        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n",
    "        self.fc = layers.Dense(3*3*512)\n",
    "\n",
    "        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [z, 100] => [z, 3*3*512]\n",
    "        x = self.fc(inputs)\n",
    "        x = tf.reshape(x, [-1, 3, 3, 512])\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        #\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.conv3(x)\n",
    "        x = tf.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator_s(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator_s, self).__init__()\n",
    "\n",
    "        # [b, 64, 64, 3] => [b, 1]\n",
    "        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        # [b, h, w ,c] => [b, -1]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        x = tf.nn.leaky_relu(self.conv1(inputs))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n",
    "\n",
    "        # [b, h, w, c] => [b, -1]\n",
    "        x = self.flatten(x)\n",
    "        # [b, -1] => [b, 1]\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def gradient_penalty(discriminator, batch_x, fake_image):\n",
    "\n",
    "    batchsz = batch_x.shape[0]\n",
    "\n",
    "    # [b, h, w, c]\n",
    "    t = tf.random.uniform([batchsz, 1, 1, 1])\n",
    "    # [b, 1, 1, 1] => [b, h, w, c]\n",
    "    t = tf.broadcast_to(t, batch_x.shape)\n",
    "\n",
    "    interplate = t * batch_x + (1 - t) * fake_image\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([interplate])\n",
    "        d_interplote_logits = discriminator(interplate, training=True)\n",
    "    grads = tape.gradient(d_interplote_logits, interplate)\n",
    "\n",
    "    # grads:[b, h, w, c] => [b, -1]\n",
    "    grads = tf.reshape(grads, [grads.shape[0], -1])\n",
    "    gp = tf.norm(grads, axis=1) #[b]\n",
    "    gp = tf.reduce_mean( (gp-1)**2 )\n",
    "\n",
    "    return gp\n",
    "\n",
    "# WGAN has different loss functions for generator and discriminator. \n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    # 1. treat real image as real\n",
    "    # 2. treat generated image as fake\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    loss = tf.reduce_mean(d_fake_logits) - tf.reduce_mean(d_real_logits)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    loss = -tf.reduce_mean(d_fake_logits)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN\n",
    "def main():\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        generator = Generator_s()\n",
    "        generator.build(input_shape = (None, z_dim))\n",
    "        # generator.summary()\n",
    "\n",
    "        discriminator = Discriminator_s()\n",
    "        discriminator.build(input_shape = (None, 64, 64, 3))\n",
    "        # discriminator.summary()\n",
    "\n",
    "    # Not to use momentum-based optimizers for WGAN, such as Adam. In contrast, use RMSProp or SGD\n",
    "        # g_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "        # d_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "        g_opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        d_opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        #g_opt = optimizers.RMSProp\n",
    "\n",
    "        d_losses, g_losses = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(5):\n",
    "                batch_z = get_random_z(z_dim, batch_size)\n",
    "                batch_x = next(db_iter)\n",
    "\n",
    "                with tf.GradientTape() as d_tape:\n",
    "                    #fake_images = generator(batch_z, training = True)\n",
    "                    #fake_outputs = discriminator(fake_images, training = True)\n",
    "                    #real_outputs = discriminator(batch_x, training = True)\n",
    "                    d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "                    #d_loss = tf.reduce_mean(fake_outputs) - tf.reduce_mean(real_outputs)\n",
    "\n",
    "                # One important difference between DCGAN and WGAN is the clipping process\n",
    "                # that clip the weights of Discriminator into a range (e.g. -0.01 - 0.01)\n",
    "                # every iteration.\n",
    "                [p.assign(tf.clip_by_value(p,-0.01,0.01)) for p in discriminator.trainable_variables]\n",
    "                d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "                d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "\n",
    "                batch_zz = get_random_z(z_dim, batch_size)\n",
    "\n",
    "                with tf.GradientTape() as g_tape:\n",
    "                    #fake_outputs = discriminator(generator(batch_zz, training=True), training=True)\n",
    "                    #g_loss = -tf.reduce_mean(fake_outputs)\n",
    "                    g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "\n",
    "                g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "                g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "\n",
    "            if epoch %100 ==0:\n",
    "                print(\"Epoch:\", epoch, \"D-loss:\", float(d_loss), \"G-loss:\", float(g_loss))            \n",
    "                z = tf.random.uniform([100, z_dim], minval=-1, maxval=1)\n",
    "                generated_images = generator(z, training = False)\n",
    "                img_path = os.path.join('gan_images', 'wgan-dcgan-%d.png'%epoch)\n",
    "                save_result(generated_images.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "                d_losses.append(float(d_loss))\n",
    "                g_losses.append(float(g_loss))\n",
    "\n",
    "            if epoch % 10000 == 1:\n",
    "                generator.save_weights('generator.ckpt')\n",
    "                discriminator.save_weights('discriminator.ckpt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 d-loss: 1.1007704734802246 g-loss: -0.4632747173309326 gp: 0.10838565975427628\n",
      "100 d-loss: -0.7238081693649292 g-loss: -0.6365422010421753 gp: 0.021410498768091202\n",
      "200 d-loss: -0.5758966207504272 g-loss: -1.0889592170715332 gp: 0.0505334734916687\n",
      "300 d-loss: -0.0852719247341156 g-loss: -0.9664949178695679 gp: 0.026621194556355476\n",
      "400 d-loss: -0.2409249246120453 g-loss: -1.0052525997161865 gp: 0.01939362660050392\n",
      "500 d-loss: -0.06857191026210785 g-loss: -0.8566693663597107 gp: 0.019396914169192314\n",
      "600 d-loss: -0.06188768148422241 g-loss: -0.7952290773391724 gp: 0.021045386791229248\n",
      "700 d-loss: -0.2486681193113327 g-loss: -0.7828883528709412 gp: 0.009191693738102913\n",
      "800 d-loss: -0.11653535068035126 g-loss: -0.8499849438667297 gp: 0.011796565726399422\n",
      "900 d-loss: -0.2219536006450653 g-loss: -0.837039053440094 gp: 0.005095598753541708\n",
      "1000 d-loss: -0.17695215344429016 g-loss: -0.802977442741394 gp: 0.006519088987261057\n",
      "1100 d-loss: -0.09535479545593262 g-loss: -0.9423953890800476 gp: 0.007483136374503374\n",
      "1200 d-loss: -0.08192583918571472 g-loss: -0.7451643347740173 gp: 0.01757963001728058\n",
      "1300 d-loss: -0.05517004802823067 g-loss: -0.7472108602523804 gp: 0.005717743653804064\n",
      "1400 d-loss: -0.13389787077903748 g-loss: -0.6692366003990173 gp: 0.010633077472448349\n",
      "1500 d-loss: -0.2866573631763458 g-loss: -0.6527813673019409 gp: 0.005981264635920525\n",
      "1600 d-loss: -0.17072001099586487 g-loss: -0.7315770387649536 gp: 0.008131756447255611\n",
      "1700 d-loss: -0.12821634113788605 g-loss: -0.7402480840682983 gp: 0.013583074323832989\n",
      "1800 d-loss: -0.09348614513874054 g-loss: -0.7837902307510376 gp: 0.006885533221065998\n",
      "1900 d-loss: -0.0758410319685936 g-loss: -0.7742633819580078 gp: 0.010271922685205936\n",
      "2000 d-loss: -0.2669661045074463 g-loss: -0.7176177501678467 gp: 0.004996918607503176\n",
      "2100 d-loss: -0.22718849778175354 g-loss: -0.7255452275276184 gp: 0.007674047723412514\n",
      "2200 d-loss: -0.22184184193611145 g-loss: -0.6606588363647461 gp: 0.006334926933050156\n",
      "2300 d-loss: -0.12366731464862823 g-loss: -0.8648495674133301 gp: 0.005321006290614605\n",
      "2400 d-loss: -0.10004853457212448 g-loss: -0.8947306871414185 gp: 0.004841101821511984\n",
      "2500 d-loss: -0.22945904731750488 g-loss: -0.5908662676811218 gp: 0.005362857133150101\n",
      "2600 d-loss: -0.17490510642528534 g-loss: -0.7093180418014526 gp: 0.0064391447231173515\n",
      "2700 d-loss: -0.17490756511688232 g-loss: -0.6613153219223022 gp: 0.004583286587148905\n",
      "2800 d-loss: -0.22023239731788635 g-loss: -0.6247867345809937 gp: 0.003531680442392826\n",
      "2900 d-loss: -0.2839090824127197 g-loss: -0.4865405857563019 gp: 0.00570498313754797\n",
      "3000 d-loss: -0.40088021755218506 g-loss: -0.5114231705665588 gp: 0.006501638796180487\n",
      "3100 d-loss: -0.27812716364860535 g-loss: -0.21320442855358124 gp: 0.012416929937899113\n",
      "3200 d-loss: -0.5340748429298401 g-loss: -0.23081094026565552 gp: 0.0068705715239048\n",
      "3300 d-loss: -0.35945114493370056 g-loss: -0.21709094941616058 gp: 0.006242732051759958\n",
      "3400 d-loss: -0.37350237369537354 g-loss: -0.40577831864356995 gp: 0.013140720315277576\n",
      "3500 d-loss: -0.14714889228343964 g-loss: -0.42530420422554016 gp: 0.007686395663768053\n",
      "3600 d-loss: -0.5532534122467041 g-loss: -0.28262895345687866 gp: 0.01434879470616579\n",
      "3700 d-loss: -0.3593798875808716 g-loss: -0.582112193107605 gp: 0.007038719952106476\n",
      "3800 d-loss: -0.3635757565498352 g-loss: -0.31781691312789917 gp: 0.006955459713935852\n",
      "3900 d-loss: -0.41269782185554504 g-loss: -0.09290780127048492 gp: 0.007095912005752325\n",
      "4000 d-loss: -0.546448826789856 g-loss: -0.030787393450737 gp: 0.010221126489341259\n",
      "4100 d-loss: -0.4022863805294037 g-loss: -0.29279112815856934 gp: 0.00939957145601511\n",
      "4200 d-loss: -0.45927172899246216 g-loss: -0.2260657697916031 gp: 0.009999086149036884\n",
      "4300 d-loss: -0.4545908570289612 g-loss: -0.08840145170688629 gp: 0.005709034390747547\n",
      "4400 d-loss: -0.4818000793457031 g-loss: -0.48809814453125 gp: 0.00675237225368619\n",
      "4500 d-loss: -0.47842180728912354 g-loss: -0.48371684551239014 gp: 0.009622873738408089\n",
      "4600 d-loss: -0.5487586259841919 g-loss: -0.1699492335319519 gp: 0.008282914757728577\n",
      "4700 d-loss: -0.5225443840026855 g-loss: -0.417021244764328 gp: 0.01003518421202898\n",
      "4800 d-loss: -0.45905154943466187 g-loss: -0.9142007827758789 gp: 0.008821161463856697\n",
      "4900 d-loss: -0.48381638526916504 g-loss: -0.437823623418808 gp: 0.005879916250705719\n",
      "5000 d-loss: -0.34433019161224365 g-loss: -0.5386838912963867 gp: 0.008844278752803802\n",
      "5100 d-loss: -0.6199162602424622 g-loss: -0.48228782415390015 gp: 0.011850054375827312\n",
      "5200 d-loss: -0.6513412594795227 g-loss: -0.5674945116043091 gp: 0.0065546040423214436\n",
      "5300 d-loss: -0.6982492804527283 g-loss: -0.7840585112571716 gp: 0.012242885306477547\n",
      "5400 d-loss: -0.4675939977169037 g-loss: -0.3410283029079437 gp: 0.006701127626001835\n",
      "5500 d-loss: -0.5556969046592712 g-loss: -0.6034800410270691 gp: 0.010380886495113373\n",
      "5600 d-loss: -0.546154260635376 g-loss: -1.0636708736419678 gp: 0.008886813186109066\n",
      "5700 d-loss: -0.6353505849838257 g-loss: -1.2743144035339355 gp: 0.006189355626702309\n",
      "5800 d-loss: -0.37083369493484497 g-loss: -1.3624613285064697 gp: 0.0061564152128994465\n",
      "5900 d-loss: -0.5471720695495605 g-loss: -1.0825066566467285 gp: 0.00859597697854042\n",
      "6000 d-loss: -0.8525978922843933 g-loss: -0.3606739044189453 gp: 0.008161826059222221\n",
      "6100 d-loss: -0.3368511199951172 g-loss: -1.3116098642349243 gp: 0.017354106530547142\n",
      "6200 d-loss: -0.7232940196990967 g-loss: -0.7590848803520203 gp: 0.0071989428251981735\n",
      "6300 d-loss: -0.7638142108917236 g-loss: -0.9136809706687927 gp: 0.008855262771248817\n",
      "6400 d-loss: -0.7352790236473083 g-loss: -0.9325864911079407 gp: 0.008034891448915005\n",
      "6500 d-loss: -0.8766516447067261 g-loss: -0.9473778009414673 gp: 0.008168389089405537\n",
      "6600 d-loss: -0.5854912400245667 g-loss: -1.2517666816711426 gp: 0.011853940784931183\n",
      "6700 d-loss: -0.7780212163925171 g-loss: -1.2806317806243896 gp: 0.010115232318639755\n",
      "6800 d-loss: -0.7016510963439941 g-loss: -1.0554664134979248 gp: 0.00631427438929677\n",
      "6900 d-loss: -0.7414326071739197 g-loss: -1.394689679145813 gp: 0.0055975522845983505\n",
      "7000 d-loss: -0.6719455718994141 g-loss: -1.920882225036621 gp: 0.01058968435972929\n",
      "7100 d-loss: -0.9741382598876953 g-loss: -0.8973231315612793 gp: 0.01158211100846529\n",
      "7200 d-loss: -0.8608940839767456 g-loss: -1.6273643970489502 gp: 0.007349122781306505\n",
      "7300 d-loss: -0.8623882532119751 g-loss: -1.8751447200775146 gp: 0.015965690836310387\n",
      "7400 d-loss: -0.6412867307662964 g-loss: -1.4727699756622314 gp: 0.00921111274510622\n",
      "7500 d-loss: -0.7436269521713257 g-loss: -2.1464319229125977 gp: 0.009632411412894726\n",
      "7600 d-loss: -0.9747902154922485 g-loss: -1.712756872177124 gp: 0.008744442835450172\n",
      "7700 d-loss: -0.8425527215003967 g-loss: -2.161116123199463 gp: 0.012857286259531975\n",
      "7800 d-loss: -0.6920396089553833 g-loss: -1.9258021116256714 gp: 0.008605161681771278\n",
      "7900 d-loss: -0.6942295432090759 g-loss: -1.5546048879623413 gp: 0.007750228978693485\n",
      "8000 d-loss: -0.7371531128883362 g-loss: -1.5750030279159546 gp: 0.0092533640563488\n",
      "8100 d-loss: -1.0236542224884033 g-loss: -1.2723329067230225 gp: 0.00786479189991951\n",
      "8200 d-loss: -0.9753831624984741 g-loss: -2.0255608558654785 gp: 0.008270349353551865\n",
      "8300 d-loss: -0.7398405075073242 g-loss: -1.8313989639282227 gp: 0.007756492123007774\n",
      "8400 d-loss: -0.9546495079994202 g-loss: -1.4436774253845215 gp: 0.01180390827357769\n",
      "8500 d-loss: -0.7787242531776428 g-loss: -1.7876498699188232 gp: 0.014134161174297333\n",
      "8600 d-loss: -1.0424269437789917 g-loss: -2.158979654312134 gp: 0.007572658360004425\n",
      "8700 d-loss: -1.167777419090271 g-loss: -1.8550339937210083 gp: 0.010308394208550453\n",
      "8800 d-loss: -1.0283863544464111 g-loss: -1.7751072645187378 gp: 0.014726085588335991\n",
      "8900 d-loss: -0.8705710768699646 g-loss: -2.9539787769317627 gp: 0.008670669049024582\n",
      "9000 d-loss: -0.9438105225563049 g-loss: -2.474477767944336 gp: 0.009254639968276024\n",
      "9100 d-loss: -1.0101847648620605 g-loss: -1.692282795906067 gp: 0.015339807607233524\n",
      "9200 d-loss: -0.6449479460716248 g-loss: -2.4516122341156006 gp: 0.01621914654970169\n",
      "9300 d-loss: -0.9724408984184265 g-loss: -1.913309931755066 gp: 0.0077492548152804375\n",
      "9400 d-loss: -0.929359495639801 g-loss: -2.146615982055664 gp: 0.006542891263961792\n",
      "9500 d-loss: -0.9331737160682678 g-loss: -1.966576099395752 gp: 0.009008483961224556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600 d-loss: -1.0403869152069092 g-loss: -1.953627109527588 gp: 0.008685003966093063\n",
      "9700 d-loss: -1.036900520324707 g-loss: -1.6760950088500977 gp: 0.006835062988102436\n",
      "9800 d-loss: -0.6649479866027832 g-loss: -1.3436706066131592 gp: 0.012149062007665634\n",
      "9900 d-loss: -0.6027340888977051 g-loss: -2.3557980060577393 gp: 0.012798193842172623\n",
      "10000 d-loss: -1.176830768585205 g-loss: -1.5072407722473145 gp: 0.007564421743154526\n",
      "10100 d-loss: -1.2576117515563965 g-loss: -2.0789952278137207 gp: 0.006364081054925919\n",
      "10200 d-loss: -0.7990856766700745 g-loss: -2.586362600326538 gp: 0.018680088222026825\n",
      "10300 d-loss: -0.7625638246536255 g-loss: -1.738723635673523 gp: 0.012685154564678669\n",
      "10400 d-loss: -0.8597558736801147 g-loss: -2.8692307472229004 gp: 0.009707987308502197\n",
      "10500 d-loss: -1.352446436882019 g-loss: -1.60464346408844 gp: 0.009247985668480396\n",
      "10600 d-loss: -0.9403713345527649 g-loss: -2.4664418697357178 gp: 0.01044723391532898\n",
      "10700 d-loss: -1.0330400466918945 g-loss: -2.896320104598999 gp: 0.015049094334244728\n",
      "10800 d-loss: -0.6764838695526123 g-loss: -1.847819447517395 gp: 0.011256316676735878\n",
      "10900 d-loss: -0.78882896900177 g-loss: -3.081803560256958 gp: 0.010318337008357048\n",
      "11000 d-loss: -0.9510692358016968 g-loss: -2.04852032661438 gp: 0.008131278678774834\n",
      "11100 d-loss: -1.0352082252502441 g-loss: -2.3836612701416016 gp: 0.00951264426112175\n",
      "11200 d-loss: -0.7274343371391296 g-loss: -1.890470027923584 gp: 0.007390483748167753\n",
      "11300 d-loss: -1.0445888042449951 g-loss: -2.242894411087036 gp: 0.007462817709892988\n",
      "11400 d-loss: -0.783024787902832 g-loss: -3.4484245777130127 gp: 0.014905357733368874\n",
      "11500 d-loss: -0.9063087701797485 g-loss: -2.5915985107421875 gp: 0.008790862746536732\n",
      "11600 d-loss: -0.9403199553489685 g-loss: -2.3304755687713623 gp: 0.011875970289111137\n",
      "11700 d-loss: -1.346790075302124 g-loss: -1.9499471187591553 gp: 0.00778193399310112\n",
      "11800 d-loss: -0.9357556700706482 g-loss: -1.5395632982254028 gp: 0.012866409495472908\n",
      "11900 d-loss: -1.0990573167800903 g-loss: -3.3567864894866943 gp: 0.025532737374305725\n",
      "12000 d-loss: -0.9869849681854248 g-loss: -1.3575925827026367 gp: 0.009268511086702347\n",
      "12100 d-loss: -1.276343584060669 g-loss: -1.1323206424713135 gp: 0.013082929886877537\n",
      "12200 d-loss: -1.1146893501281738 g-loss: -2.5778462886810303 gp: 0.014888519421219826\n",
      "12300 d-loss: -0.8073263168334961 g-loss: -2.1388237476348877 gp: 0.007673953659832478\n",
      "12400 d-loss: -0.8722453117370605 g-loss: -2.4170949459075928 gp: 0.009452488273382187\n",
      "12500 d-loss: -1.1057491302490234 g-loss: -2.7373290061950684 gp: 0.017094559967517853\n",
      "12600 d-loss: -0.9672448039054871 g-loss: -2.3801112174987793 gp: 0.007447417825460434\n",
      "12700 d-loss: -1.0082974433898926 g-loss: -1.7669726610183716 gp: 0.016360042616724968\n",
      "12800 d-loss: -0.8831213116645813 g-loss: -3.259950876235962 gp: 0.017031211405992508\n",
      "12900 d-loss: -1.12596595287323 g-loss: -2.803332805633545 gp: 0.011663234792649746\n",
      "13000 d-loss: -0.9412326216697693 g-loss: -2.3602967262268066 gp: 0.008633619174361229\n",
      "13100 d-loss: -0.5665717124938965 g-loss: -1.5795915126800537 gp: 0.029842279851436615\n",
      "13200 d-loss: -0.929253339767456 g-loss: -2.481473207473755 gp: 0.008870264515280724\n",
      "13300 d-loss: -0.7370863556861877 g-loss: -2.9759316444396973 gp: 0.00914585217833519\n",
      "13400 d-loss: -1.133439302444458 g-loss: -2.815234661102295 gp: 0.01596665196120739\n",
      "13500 d-loss: -1.00568687915802 g-loss: -3.356987237930298 gp: 0.010833822190761566\n",
      "13600 d-loss: -0.8590792417526245 g-loss: -3.6313071250915527 gp: 0.009859338402748108\n",
      "13700 d-loss: -1.4515174627304077 g-loss: -2.8614792823791504 gp: 0.016612686216831207\n",
      "13800 d-loss: -0.9497036933898926 g-loss: -2.5762856006622314 gp: 0.007987404242157936\n",
      "13900 d-loss: -1.2461684942245483 g-loss: -2.3546836376190186 gp: 0.012529744766652584\n",
      "14000 d-loss: -1.4271869659423828 g-loss: -2.561521053314209 gp: 0.00977303460240364\n",
      "14100 d-loss: -1.0631816387176514 g-loss: -3.3326869010925293 gp: 0.008202765136957169\n",
      "14200 d-loss: -0.957953691482544 g-loss: -3.1319241523742676 gp: 0.016887927427887917\n",
      "14300 d-loss: -0.8100047707557678 g-loss: -2.2255377769470215 gp: 0.012674348428845406\n",
      "14400 d-loss: -1.1001050472259521 g-loss: -4.207252502441406 gp: 0.009710576385259628\n",
      "14500 d-loss: -0.9784290790557861 g-loss: -3.145838499069214 gp: 0.009454205632209778\n",
      "14600 d-loss: -0.6347424983978271 g-loss: -3.5583460330963135 gp: 0.017424222081899643\n",
      "14700 d-loss: -1.0250540971755981 g-loss: -3.4971959590911865 gp: 0.00681904423981905\n",
      "14800 d-loss: -1.1040613651275635 g-loss: -3.5109102725982666 gp: 0.011345380917191505\n",
      "14900 d-loss: -1.0060789585113525 g-loss: -2.87170147895813 gp: 0.008930227719247341\n",
      "15000 d-loss: -1.1498292684555054 g-loss: -3.70828914642334 gp: 0.00942140445113182\n",
      "15100 d-loss: -1.0961637496948242 g-loss: -2.7090628147125244 gp: 0.012160247191786766\n",
      "15200 d-loss: -1.1034185886383057 g-loss: -3.2378807067871094 gp: 0.010545474477112293\n",
      "15300 d-loss: -0.8629148602485657 g-loss: -2.8133277893066406 gp: 0.009177514351904392\n",
      "15400 d-loss: -0.8349027037620544 g-loss: -2.222233772277832 gp: 0.008720763027668\n",
      "15500 d-loss: -0.6760319471359253 g-loss: -3.2212371826171875 gp: 0.017356455326080322\n",
      "15600 d-loss: -0.9999977946281433 g-loss: -4.189854145050049 gp: 0.010431415401399136\n",
      "15700 d-loss: -0.7776576280593872 g-loss: -2.985797166824341 gp: 0.007893310859799385\n",
      "15800 d-loss: -0.9733233451843262 g-loss: -3.062490224838257 gp: 0.007364797871559858\n",
      "15900 d-loss: -0.6903004050254822 g-loss: -2.3906431198120117 gp: 0.008800800889730453\n",
      "16000 d-loss: -0.8611741065979004 g-loss: -3.771026611328125 gp: 0.009528925642371178\n",
      "16100 d-loss: -0.9924477934837341 g-loss: -3.6674644947052 gp: 0.015491155907511711\n",
      "16200 d-loss: -0.6840579509735107 g-loss: -3.663145065307617 gp: 0.00934114120900631\n",
      "16300 d-loss: -0.990554690361023 g-loss: -3.902961254119873 gp: 0.008566534146666527\n",
      "16400 d-loss: -0.9678394794464111 g-loss: -4.039140701293945 gp: 0.007803607266396284\n",
      "16500 d-loss: -1.0716688632965088 g-loss: -2.585535764694214 gp: 0.008784228935837746\n",
      "16600 d-loss: -1.1232571601867676 g-loss: -3.4527578353881836 gp: 0.013666898012161255\n",
      "16700 d-loss: -0.9842268228530884 g-loss: -4.083767890930176 gp: 0.008944215252995491\n",
      "16800 d-loss: -0.6487950086593628 g-loss: -4.25352668762207 gp: 0.010480460710823536\n",
      "16900 d-loss: -1.1803382635116577 g-loss: -3.1085903644561768 gp: 0.006588352378457785\n",
      "17000 d-loss: -0.986081600189209 g-loss: -2.8322203159332275 gp: 0.0062146419659256935\n",
      "17100 d-loss: -1.2454838752746582 g-loss: -3.6329145431518555 gp: 0.020777128636837006\n",
      "17200 d-loss: -0.8640654683113098 g-loss: -4.145995140075684 gp: 0.011048050597310066\n",
      "17300 d-loss: -0.9331238865852356 g-loss: -3.011504888534546 gp: 0.008801070973277092\n",
      "17400 d-loss: -0.8961843252182007 g-loss: -3.5023865699768066 gp: 0.01053694635629654\n",
      "17500 d-loss: -1.1425167322158813 g-loss: -3.4216697216033936 gp: 0.013529196381568909\n",
      "17600 d-loss: -1.0037521123886108 g-loss: -3.1249618530273438 gp: 0.010848910547792912\n",
      "17700 d-loss: -1.1540066003799438 g-loss: -2.4716453552246094 gp: 0.008534940890967846\n",
      "17800 d-loss: -1.3695032596588135 g-loss: -3.7393102645874023 gp: 0.012706918641924858\n",
      "17900 d-loss: -0.9551519155502319 g-loss: -3.067091941833496 gp: 0.01568235456943512\n",
      "18000 d-loss: -1.2169286012649536 g-loss: -2.242133140563965 gp: 0.009916720911860466\n",
      "18100 d-loss: -1.059792399406433 g-loss: -3.3230557441711426 gp: 0.015031639486551285\n",
      "18200 d-loss: -1.3095753192901611 g-loss: -4.269256114959717 gp: 0.014322401955723763\n",
      "18300 d-loss: -1.1669235229492188 g-loss: -3.65876841545105 gp: 0.010994288139045238\n",
      "18400 d-loss: -0.8944720029830933 g-loss: -3.0248444080352783 gp: 0.008536710403859615\n",
      "18500 d-loss: -1.1229935884475708 g-loss: -3.2021446228027344 gp: 0.008882553316652775\n",
      "18600 d-loss: -1.2459641695022583 g-loss: -3.7781143188476562 gp: 0.01001790538430214\n",
      "18700 d-loss: -0.9891257882118225 g-loss: -4.192383766174316 gp: 0.011554761789739132\n",
      "18800 d-loss: -1.2479541301727295 g-loss: -4.115694999694824 gp: 0.015449495054781437\n",
      "18900 d-loss: -1.2060625553131104 g-loss: -4.111296653747559 gp: 0.011394958943128586\n",
      "19000 d-loss: -1.1844348907470703 g-loss: -3.0621705055236816 gp: 0.005798506550490856\n",
      "19100 d-loss: -1.0076675415039062 g-loss: -4.206994533538818 gp: 0.008860735222697258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19200 d-loss: -0.957658052444458 g-loss: -2.8267526626586914 gp: 0.01817009411752224\n",
      "19300 d-loss: -1.035115122795105 g-loss: -3.991234064102173 gp: 0.012524373829364777\n",
      "19400 d-loss: -1.0102733373641968 g-loss: -3.2674243450164795 gp: 0.009046114049851894\n",
      "19500 d-loss: -1.354128122329712 g-loss: -3.1218760013580322 gp: 0.012844866141676903\n",
      "19600 d-loss: -0.9791883230209351 g-loss: -2.8349249362945557 gp: 0.012695420533418655\n",
      "19700 d-loss: -1.357460379600525 g-loss: -2.9970717430114746 gp: 0.016504153609275818\n",
      "19800 d-loss: -0.8513644337654114 g-loss: -3.7471365928649902 gp: 0.008139201439917088\n",
      "19900 d-loss: -1.0996615886688232 g-loss: -3.3166122436523438 gp: 0.01146185677498579\n",
      "20000 d-loss: -0.8975555300712585 g-loss: -3.582353353500366 gp: 0.006888802163302898\n",
      "20100 d-loss: -1.0538487434387207 g-loss: -3.7024006843566895 gp: 0.009463215246796608\n",
      "20200 d-loss: -1.2296863794326782 g-loss: -3.0002927780151367 gp: 0.010037627071142197\n",
      "20300 d-loss: -1.3445062637329102 g-loss: -4.0735368728637695 gp: 0.011521343141794205\n",
      "20400 d-loss: -1.2496813535690308 g-loss: -3.5245285034179688 gp: 0.02077447436749935\n",
      "20500 d-loss: -1.0139780044555664 g-loss: -4.185015678405762 gp: 0.009891982190310955\n",
      "20600 d-loss: -1.0704962015151978 g-loss: -3.324542284011841 gp: 0.006467802450060844\n",
      "20700 d-loss: -1.1055245399475098 g-loss: -3.5516088008880615 gp: 0.008087258785963058\n",
      "20800 d-loss: -0.7281764149665833 g-loss: -3.6812522411346436 gp: 0.017522305250167847\n",
      "20900 d-loss: -1.0029696226119995 g-loss: -4.190237522125244 gp: 0.009372820146381855\n",
      "21000 d-loss: -0.8106626868247986 g-loss: -4.963015079498291 gp: 0.009022957645356655\n",
      "21100 d-loss: -0.9522626399993896 g-loss: -3.8395028114318848 gp: 0.012662052176892757\n",
      "21200 d-loss: -0.9961767196655273 g-loss: -4.501106262207031 gp: 0.024550197646021843\n",
      "21300 d-loss: -1.0949938297271729 g-loss: -4.139966011047363 gp: 0.009193828329443932\n",
      "21400 d-loss: -1.135359764099121 g-loss: -3.866147994995117 gp: 0.008809617720544338\n",
      "21500 d-loss: -1.351446270942688 g-loss: -3.916412115097046 gp: 0.011544736102223396\n",
      "21600 d-loss: -1.121176838874817 g-loss: -2.3380661010742188 gp: 0.013207690790295601\n",
      "21700 d-loss: -0.8981467485427856 g-loss: -3.120875835418701 gp: 0.007762900087982416\n",
      "21800 d-loss: -0.9157100915908813 g-loss: -3.249720573425293 gp: 0.011743316426873207\n",
      "21900 d-loss: -1.1251587867736816 g-loss: -3.2965564727783203 gp: 0.013301510363817215\n",
      "22000 d-loss: -1.0421783924102783 g-loss: -4.271440505981445 gp: 0.016315001994371414\n",
      "22100 d-loss: -0.7986789345741272 g-loss: -3.7970731258392334 gp: 0.010209601372480392\n",
      "22200 d-loss: -1.109002709388733 g-loss: -4.432854175567627 gp: 0.010225235484540462\n",
      "22300 d-loss: -0.8824285864830017 g-loss: -4.968975067138672 gp: 0.014431722462177277\n",
      "22400 d-loss: -1.2449644804000854 g-loss: -3.989222526550293 gp: 0.017601313069462776\n",
      "22500 d-loss: -1.203710913658142 g-loss: -3.2544260025024414 gp: 0.011113421060144901\n",
      "22600 d-loss: -0.9703595638275146 g-loss: -3.34679913520813 gp: 0.008224820718169212\n",
      "22700 d-loss: -0.9623998999595642 g-loss: -2.840567111968994 gp: 0.007125980220735073\n",
      "22800 d-loss: -1.215059518814087 g-loss: -3.8195419311523438 gp: 0.01639731414616108\n",
      "22900 d-loss: -1.2360481023788452 g-loss: -4.97463321685791 gp: 0.009241543710231781\n",
      "23000 d-loss: -1.298719882965088 g-loss: -4.027671813964844 gp: 0.010304396972060204\n",
      "23100 d-loss: -1.333747148513794 g-loss: -3.7670252323150635 gp: 0.010518530383706093\n",
      "23200 d-loss: -1.2843661308288574 g-loss: -4.1457133293151855 gp: 0.023446321487426758\n",
      "23300 d-loss: -1.3969814777374268 g-loss: -3.1161274909973145 gp: 0.008170366287231445\n",
      "23400 d-loss: -1.1794544458389282 g-loss: -3.547070026397705 gp: 0.00959023181349039\n",
      "23500 d-loss: -1.2171145677566528 g-loss: -2.802293062210083 gp: 0.008116018027067184\n",
      "23600 d-loss: -1.0634796619415283 g-loss: -4.457900524139404 gp: 0.006615429185330868\n",
      "23700 d-loss: -1.3415718078613281 g-loss: -3.1387932300567627 gp: 0.010898089967668056\n",
      "23800 d-loss: -0.9040093421936035 g-loss: -4.674075126647949 gp: 0.02079630270600319\n",
      "23900 d-loss: -1.0288832187652588 g-loss: -4.412080764770508 gp: 0.007889965549111366\n",
      "24000 d-loss: -1.2569997310638428 g-loss: -4.457790374755859 gp: 0.01038872916251421\n",
      "24100 d-loss: -0.6956741809844971 g-loss: -4.999539375305176 gp: 0.02646172046661377\n",
      "24200 d-loss: -0.9331821203231812 g-loss: -3.3575973510742188 gp: 0.00819348357617855\n",
      "24300 d-loss: -1.244251012802124 g-loss: -4.390830039978027 gp: 0.008268661797046661\n",
      "24400 d-loss: -1.291594386100769 g-loss: -3.4616541862487793 gp: 0.00986863486468792\n",
      "24500 d-loss: -0.8753534555435181 g-loss: -4.047799587249756 gp: 0.007658231537789106\n",
      "24600 d-loss: -0.8729013204574585 g-loss: -3.942882537841797 gp: 0.01732412539422512\n",
      "24700 d-loss: -0.9632431268692017 g-loss: -3.3121182918548584 gp: 0.022655662149190903\n",
      "24800 d-loss: -1.376592755317688 g-loss: -4.418715000152588 gp: 0.008075226098299026\n",
      "24900 d-loss: -1.286795973777771 g-loss: -3.4406330585479736 gp: 0.01202303171157837\n",
      "25000 d-loss: -1.3242318630218506 g-loss: -3.889033079147339 gp: 0.010271187871694565\n",
      "25100 d-loss: -1.014185905456543 g-loss: -4.215112209320068 gp: 0.014651864767074585\n",
      "25200 d-loss: -0.9036964178085327 g-loss: -4.165682315826416 gp: 0.02215481735765934\n",
      "25300 d-loss: -1.2524996995925903 g-loss: -4.171004772186279 gp: 0.011665524914860725\n",
      "25400 d-loss: -1.5844230651855469 g-loss: -3.856945037841797 gp: 0.006533190608024597\n",
      "25500 d-loss: -1.213805913925171 g-loss: -3.780099868774414 gp: 0.008393811993300915\n",
      "25600 d-loss: -0.6906054615974426 g-loss: -4.497730731964111 gp: 0.0169110968708992\n",
      "25700 d-loss: -1.2040034532546997 g-loss: -4.240289688110352 gp: 0.018281681463122368\n",
      "25800 d-loss: -1.4017778635025024 g-loss: -3.3145623207092285 gp: 0.008860986679792404\n",
      "25900 d-loss: -1.149122953414917 g-loss: -4.09144401550293 gp: 0.0077084824442863464\n",
      "26000 d-loss: -1.357001543045044 g-loss: -4.72298526763916 gp: 0.006999614182859659\n",
      "26100 d-loss: -0.9692199230194092 g-loss: -3.8437156677246094 gp: 0.01711454428732395\n",
      "26200 d-loss: -1.1768025159835815 g-loss: -4.442686080932617 gp: 0.008866516873240471\n",
      "26300 d-loss: -0.7208497524261475 g-loss: -3.1891651153564453 gp: 0.012893319129943848\n",
      "26400 d-loss: -1.0475716590881348 g-loss: -3.6506009101867676 gp: 0.008688635192811489\n",
      "26500 d-loss: -1.2933032512664795 g-loss: -3.6555631160736084 gp: 0.010050559416413307\n",
      "26600 d-loss: -1.2167284488677979 g-loss: -3.912646532058716 gp: 0.011323550716042519\n",
      "26700 d-loss: -1.5900360345840454 g-loss: -4.049744606018066 gp: 0.012433996424078941\n",
      "26800 d-loss: -1.561669111251831 g-loss: -4.48358678817749 gp: 0.00730779767036438\n",
      "26900 d-loss: -0.928562343120575 g-loss: -3.772510528564453 gp: 0.009621839039027691\n",
      "27000 d-loss: -1.147848129272461 g-loss: -3.6101980209350586 gp: 0.013110114261507988\n",
      "27100 d-loss: -1.1572884321212769 g-loss: -4.013268947601318 gp: 0.0078124175779521465\n",
      "27200 d-loss: -1.147411584854126 g-loss: -4.62459659576416 gp: 0.00847809761762619\n",
      "27300 d-loss: -1.2571746110916138 g-loss: -3.9828758239746094 gp: 0.01034099142998457\n",
      "27400 d-loss: -0.8955687284469604 g-loss: -4.286453723907471 gp: 0.016043957322835922\n",
      "27500 d-loss: -1.1187951564788818 g-loss: -4.24366569519043 gp: 0.010728384368121624\n",
      "27600 d-loss: -1.446041464805603 g-loss: -4.450505256652832 gp: 0.014625891111791134\n",
      "27700 d-loss: -1.5239970684051514 g-loss: -3.22741961479187 gp: 0.009644224308431149\n",
      "27800 d-loss: -0.7092455625534058 g-loss: -2.846092700958252 gp: 0.021783772855997086\n",
      "27900 d-loss: -1.5125091075897217 g-loss: -4.036489009857178 gp: 0.01453525759279728\n",
      "28000 d-loss: -1.0990866422653198 g-loss: -4.125733852386475 gp: 0.013406094163656235\n",
      "28100 d-loss: -0.9627769589424133 g-loss: -3.8608291149139404 gp: 0.01488687563687563\n",
      "28200 d-loss: -1.2941389083862305 g-loss: -4.204891681671143 gp: 0.013055652379989624\n",
      "28300 d-loss: -1.5631145238876343 g-loss: -3.7130749225616455 gp: 0.015451066195964813\n",
      "28400 d-loss: -0.9062961339950562 g-loss: -4.099954605102539 gp: 0.010859455913305283\n",
      "28500 d-loss: -1.293257713317871 g-loss: -4.268520832061768 gp: 0.010277364403009415\n",
      "28600 d-loss: -1.503062129020691 g-loss: -4.21346378326416 gp: 0.006996641866862774\n",
      "28700 d-loss: -1.5104882717132568 g-loss: -3.749368667602539 gp: 0.010790730826556683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28800 d-loss: -1.7908847332000732 g-loss: -4.7696452140808105 gp: 0.01963537000119686\n",
      "28900 d-loss: -0.7584096193313599 g-loss: -4.5177001953125 gp: 0.008932098746299744\n",
      "29000 d-loss: -1.4666061401367188 g-loss: -3.5395491123199463 gp: 0.012038704939186573\n",
      "29100 d-loss: -1.4551613330841064 g-loss: -4.2889604568481445 gp: 0.009584502317011356\n",
      "29200 d-loss: -1.2123228311538696 g-loss: -3.476316213607788 gp: 0.006704746745526791\n",
      "29300 d-loss: -1.6850897073745728 g-loss: -4.418304920196533 gp: 0.009497438557446003\n",
      "29400 d-loss: -0.6597468256950378 g-loss: -3.683561086654663 gp: 0.010870367288589478\n",
      "29500 d-loss: -1.0935466289520264 g-loss: -3.734968662261963 gp: 0.013593361712992191\n",
      "29600 d-loss: -1.1596962213516235 g-loss: -3.7583751678466797 gp: 0.00879429467022419\n",
      "29700 d-loss: -1.3059370517730713 g-loss: -4.289397716522217 gp: 0.011777287349104881\n",
      "29800 d-loss: -1.082399845123291 g-loss: -3.729719877243042 gp: 0.015549378469586372\n",
      "29900 d-loss: -1.2019621133804321 g-loss: -4.995218276977539 gp: 0.011600431054830551\n",
      "30000 d-loss: -1.1333938837051392 g-loss: -4.229708194732666 gp: 0.011455902829766273\n",
      "30100 d-loss: -1.3671412467956543 g-loss: -3.0596346855163574 gp: 0.009533596225082874\n",
      "30200 d-loss: -0.8479145765304565 g-loss: -4.24650764465332 gp: 0.0073761604726314545\n",
      "30300 d-loss: -0.7000504732131958 g-loss: -5.424190044403076 gp: 0.013653650879859924\n",
      "30400 d-loss: -1.776774525642395 g-loss: -3.53583025932312 gp: 0.009937127120792866\n",
      "30500 d-loss: -1.3762736320495605 g-loss: -4.564630508422852 gp: 0.019916389137506485\n",
      "30600 d-loss: -1.2430284023284912 g-loss: -5.209959983825684 gp: 0.014717740938067436\n",
      "30700 d-loss: -1.240075707435608 g-loss: -3.790079116821289 gp: 0.013933642767369747\n",
      "30800 d-loss: -1.5625276565551758 g-loss: -4.773630142211914 gp: 0.018480731174349785\n",
      "30900 d-loss: -1.4131561517715454 g-loss: -4.984054088592529 gp: 0.010460508987307549\n",
      "31000 d-loss: -1.3232816457748413 g-loss: -5.106164932250977 gp: 0.017007745802402496\n",
      "31100 d-loss: -0.9417634606361389 g-loss: -3.9094858169555664 gp: 0.012291496619582176\n",
      "31200 d-loss: -1.0316903591156006 g-loss: -3.8424317836761475 gp: 0.009063176810741425\n",
      "31300 d-loss: -1.0389857292175293 g-loss: -5.082037448883057 gp: 0.008816913701593876\n",
      "31400 d-loss: -1.3733192682266235 g-loss: -4.635607719421387 gp: 0.00948532298207283\n",
      "31500 d-loss: -1.305158257484436 g-loss: -4.284316062927246 gp: 0.011049209162592888\n",
      "31600 d-loss: -1.6597514152526855 g-loss: -4.978325366973877 gp: 0.018735699355602264\n",
      "31700 d-loss: -1.2470414638519287 g-loss: -4.382998943328857 gp: 0.013144133612513542\n",
      "31800 d-loss: -1.1134707927703857 g-loss: -4.107288360595703 gp: 0.014472120441496372\n",
      "31900 d-loss: -0.8595616817474365 g-loss: -5.29902458190918 gp: 0.010157369077205658\n",
      "32000 d-loss: -1.1207493543624878 g-loss: -3.966742753982544 gp: 0.013354647904634476\n",
      "32100 d-loss: -1.2832574844360352 g-loss: -4.340818405151367 gp: 0.015857648104429245\n",
      "32200 d-loss: -1.6567964553833008 g-loss: -3.6714704036712646 gp: 0.01584448292851448\n",
      "32300 d-loss: -1.115220308303833 g-loss: -4.0663604736328125 gp: 0.008070450276136398\n",
      "32400 d-loss: -1.261667251586914 g-loss: -4.330635070800781 gp: 0.011351725086569786\n",
      "32500 d-loss: -1.5206669569015503 g-loss: -3.9191665649414062 gp: 0.010384956374764442\n",
      "32600 d-loss: -1.0877397060394287 g-loss: -4.017092704772949 gp: 0.013473349623382092\n",
      "32700 d-loss: -0.9456642270088196 g-loss: -5.053153991699219 gp: 0.02134048379957676\n",
      "32800 d-loss: -1.5025509595870972 g-loss: -3.879014253616333 gp: 0.013361794874072075\n",
      "32900 d-loss: -1.2783637046813965 g-loss: -4.449489593505859 gp: 0.008267591707408428\n",
      "33000 d-loss: -1.1008597612380981 g-loss: -5.278541088104248 gp: 0.008636794053018093\n",
      "33100 d-loss: -1.2385717630386353 g-loss: -4.8659586906433105 gp: 0.011134989559650421\n",
      "33200 d-loss: -1.3717796802520752 g-loss: -4.836066246032715 gp: 0.014892885461449623\n",
      "33300 d-loss: -0.8829145431518555 g-loss: -5.721240997314453 gp: 0.022280313074588776\n",
      "33400 d-loss: -0.9539303779602051 g-loss: -4.331803321838379 gp: 0.010980844497680664\n",
      "33500 d-loss: -1.1914215087890625 g-loss: -4.311160087585449 gp: 0.011414574459195137\n",
      "33600 d-loss: -1.239121913909912 g-loss: -4.743846893310547 gp: 0.01060862373560667\n",
      "33700 d-loss: -1.1295809745788574 g-loss: -5.613460063934326 gp: 0.04633880406618118\n",
      "33800 d-loss: -1.3253518342971802 g-loss: -4.8318328857421875 gp: 0.012235822156071663\n",
      "33900 d-loss: -1.2680580615997314 g-loss: -4.946541786193848 gp: 0.017694633454084396\n",
      "34000 d-loss: -1.2681758403778076 g-loss: -4.680495738983154 gp: 0.015356142073869705\n",
      "34100 d-loss: -1.1693562269210815 g-loss: -3.4585676193237305 gp: 0.013169415295124054\n",
      "34200 d-loss: -1.4269583225250244 g-loss: -4.054845809936523 gp: 0.012187881395220757\n",
      "34300 d-loss: -1.2808632850646973 g-loss: -4.700064182281494 gp: 0.012297635897994041\n",
      "34400 d-loss: -1.2260377407073975 g-loss: -4.771820545196533 gp: 0.010171294212341309\n",
      "34500 d-loss: -1.5049248933792114 g-loss: -4.248236656188965 gp: 0.013979374431073666\n",
      "34600 d-loss: -1.2733170986175537 g-loss: -4.578497409820557 gp: 0.009000086225569248\n",
      "34700 d-loss: -1.1904689073562622 g-loss: -5.132864952087402 gp: 0.009703007526695728\n",
      "34800 d-loss: -1.3948997259140015 g-loss: -5.342061996459961 gp: 0.014931024983525276\n",
      "34900 d-loss: -1.3592274188995361 g-loss: -4.860948085784912 gp: 0.017280369997024536\n",
      "35000 d-loss: -1.5153510570526123 g-loss: -4.522902011871338 gp: 0.013986366800963879\n",
      "35100 d-loss: -1.2857561111450195 g-loss: -5.761866569519043 gp: 0.026103399693965912\n",
      "35200 d-loss: -1.733102798461914 g-loss: -4.035584926605225 gp: 0.007051372900605202\n",
      "35300 d-loss: -1.2290757894515991 g-loss: -4.11412239074707 gp: 0.016314851120114326\n",
      "35400 d-loss: -1.3542858362197876 g-loss: -5.0760955810546875 gp: 0.01591300219297409\n",
      "35500 d-loss: -1.014570713043213 g-loss: -4.903639793395996 gp: 0.014034746214747429\n",
      "35600 d-loss: -1.1249743700027466 g-loss: -4.031210899353027 gp: 0.023294959217309952\n",
      "35700 d-loss: -1.7980663776397705 g-loss: -4.205028533935547 gp: 0.017980698496103287\n",
      "35800 d-loss: -1.6124083995819092 g-loss: -4.455552101135254 gp: 0.009235406294465065\n",
      "35900 d-loss: -1.441939115524292 g-loss: -5.169600486755371 gp: 0.007967110723257065\n",
      "36000 d-loss: -1.4145671129226685 g-loss: -5.378830909729004 gp: 0.020059574395418167\n",
      "36100 d-loss: -1.6321969032287598 g-loss: -4.170280933380127 gp: 0.01926102116703987\n",
      "36200 d-loss: -1.0641170740127563 g-loss: -4.678095817565918 gp: 0.01457446999847889\n",
      "36300 d-loss: -1.6654422283172607 g-loss: -4.5522003173828125 gp: 0.015620164573192596\n",
      "36400 d-loss: -1.100878119468689 g-loss: -6.305004596710205 gp: 0.02629667893052101\n",
      "36500 d-loss: -1.3742637634277344 g-loss: -4.303836822509766 gp: 0.013348907232284546\n",
      "36600 d-loss: -1.4053258895874023 g-loss: -5.064795017242432 gp: 0.009708499535918236\n",
      "36700 d-loss: -1.3390462398529053 g-loss: -4.7147908210754395 gp: 0.01806614361703396\n",
      "36800 d-loss: -1.504994511604309 g-loss: -5.1723103523254395 gp: 0.008778993040323257\n",
      "36900 d-loss: -1.4043188095092773 g-loss: -4.939977169036865 gp: 0.014826342463493347\n",
      "37000 d-loss: -1.4553399085998535 g-loss: -4.637991905212402 gp: 0.012905742973089218\n",
      "37100 d-loss: -1.3526865243911743 g-loss: -5.11807918548584 gp: 0.011423293501138687\n",
      "37200 d-loss: -1.6829197406768799 g-loss: -5.712404251098633 gp: 0.006035596132278442\n",
      "37300 d-loss: -1.2646074295043945 g-loss: -5.128585338592529 gp: 0.02203340083360672\n",
      "37400 d-loss: -1.2993972301483154 g-loss: -5.352734565734863 gp: 0.010384632274508476\n",
      "37500 d-loss: -1.311057686805725 g-loss: -5.832803726196289 gp: 0.025857964530587196\n",
      "37600 d-loss: -1.2608646154403687 g-loss: -4.814988613128662 gp: 0.010054738260805607\n",
      "37700 d-loss: -1.3267624378204346 g-loss: -4.805352210998535 gp: 0.013336732983589172\n",
      "37800 d-loss: -1.1732755899429321 g-loss: -5.32981538772583 gp: 0.009248055517673492\n",
      "37900 d-loss: -1.6286373138427734 g-loss: -4.775426864624023 gp: 0.018813470378518105\n",
      "38000 d-loss: -1.188655972480774 g-loss: -5.059175491333008 gp: 0.021003661677241325\n",
      "38100 d-loss: -1.2145302295684814 g-loss: -4.760076522827148 gp: 0.01194197591394186\n",
      "38200 d-loss: -1.5149998664855957 g-loss: -4.869631767272949 gp: 0.007356361020356417\n",
      "38300 d-loss: -1.5540543794631958 g-loss: -4.649922847747803 gp: 0.012583006173372269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38400 d-loss: -1.7216334342956543 g-loss: -4.367036819458008 gp: 0.012901635840535164\n",
      "38500 d-loss: -1.2504725456237793 g-loss: -5.386634349822998 gp: 0.014136750251054764\n",
      "38600 d-loss: -1.3280891180038452 g-loss: -6.0685625076293945 gp: 0.018591077998280525\n",
      "38700 d-loss: -0.8464654684066772 g-loss: -5.051019668579102 gp: 0.009284893050789833\n",
      "38800 d-loss: -1.0385921001434326 g-loss: -5.786165237426758 gp: 0.0309922955930233\n",
      "38900 d-loss: -1.5101544857025146 g-loss: -5.344005584716797 gp: 0.01309186965227127\n",
      "39000 d-loss: -1.318481683731079 g-loss: -5.240776062011719 gp: 0.025398042052984238\n",
      "39100 d-loss: -1.44381844997406 g-loss: -4.489887237548828 gp: 0.011832917109131813\n",
      "39200 d-loss: -1.4904979467391968 g-loss: -5.539308547973633 gp: 0.009813657961785793\n",
      "39300 d-loss: -1.3814717531204224 g-loss: -5.506291389465332 gp: 0.011600198224186897\n",
      "39400 d-loss: -1.8908910751342773 g-loss: -5.237240314483643 gp: 0.017740633338689804\n",
      "39500 d-loss: -1.2666360139846802 g-loss: -4.433967590332031 gp: 0.013328158296644688\n",
      "39600 d-loss: -1.2340267896652222 g-loss: -4.906569957733154 gp: 0.01763053983449936\n",
      "39700 d-loss: -1.387912392616272 g-loss: -4.833459377288818 gp: 0.013456807471811771\n",
      "39800 d-loss: -1.6091980934143066 g-loss: -4.937384128570557 gp: 0.01810307800769806\n",
      "39900 d-loss: -1.0875587463378906 g-loss: -5.483572006225586 gp: 0.028297416865825653\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1d719675ce8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-1d719675ce8b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mbatch_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# WGAN-GP (Gradient Penalty)\n",
    "# GP aims to constrain the weights of Discriminator to be close to 1\n",
    "# The most parts of WGAN-GP are similar with WGAN.\n",
    "# Differences: No clipping, but a penalty term in D's loss\n",
    "\n",
    "def gradient_penalty(discriminator, real, fake):\n",
    "    batch_sz = real.shape[0]\n",
    "    t = tf.random.uniform([batch_sz, 1, 1, 1])\n",
    "    t = tf.broadcast_to(t, real.shape)\n",
    "    \n",
    "    interplate = t * real + (1 - t) * fake\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([interplate])\n",
    "        d_interplate_logits = discriminator(interplate, True)\n",
    "    grads = tape.gradient(d_interplate_logits, interplate)\n",
    "    \n",
    "    grads = tf.reshape(grads, [grads.shape[0], -1])\n",
    "    gp = tf.norm(grads, axis = 1)\n",
    "    gp = tf.reduce_mean((gp - 1.)**2)\n",
    "    \n",
    "    return gp\n",
    "\n",
    "def gradient_penalty2(discriminator, batch_x, fake_image):\n",
    "\n",
    "    batchsz = batch_x.shape[0]\n",
    "\n",
    "    # [b, h, w, c]\n",
    "    t = tf.random.uniform([batchsz, 1, 1, 1])\n",
    "    # [b, 1, 1, 1] => [b, h, w, c]\n",
    "    t = tf.broadcast_to(t, batch_x.shape)\n",
    "\n",
    "    interplate = t * batch_x + (1 - t) * fake_image\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([interplate])\n",
    "        d_interplote_logits = discriminator(interplate, training=True)\n",
    "    grads = tape.gradient(d_interplote_logits, interplate)\n",
    "\n",
    "    # grads:[b, h, w, c] => [b, -1]\n",
    "    grads = tf.reshape(grads, [grads.shape[0], -1])\n",
    "    gp = tf.norm(grads, axis=1) #[b]\n",
    "    gp = tf.reduce_mean( (gp-1)**2 )\n",
    "    \n",
    "    print(gp)\n",
    "\n",
    "    return gp\n",
    "\n",
    "# Override d_loss_fn(.)\n",
    "def d_loss_gp_fn(g, d, z, real, l, is_training):\n",
    "    fake_images = g(z, is_training)\n",
    "    fake_outputs = d(fake_images, is_training)\n",
    "    real_outputs = d(real, is_training)\n",
    "    \n",
    "    gp = gradient_penalty(d, real, fake_images)\n",
    "    \n",
    "    loss = tf.reduce_mean(fake_outputs) - tf.reduce_mean(real_outputs) + 10. * gp\n",
    "    \n",
    "    return loss, gp\n",
    "    \n",
    "import tensorflow as tf\n",
    "def main():\n",
    "    l = 10\n",
    "    generator = Generator_s()\n",
    "    generator.build(input_shape=(None, z_dim))\n",
    "\n",
    "    discriminator = Discriminator_s()\n",
    "    discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "\n",
    "    g_opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    d_opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "#    for epoch in range(epochs):\n",
    "#        for _ in range(5):\n",
    "#            batch_z = get_random_z(z_dim, batch_size)\n",
    "#            batch_x = next(db_iter)\n",
    "#\n",
    "#            with tf.GradientTape() as d_tape:\n",
    "#                d_loss, gp = d_loss_gp_fn(generator, discriminator, batch_z, batch_x, l, is_training)\n",
    "#            d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "#            d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "#\n",
    "#        batch_zz = get_random_z(z_dim, batch_size)\n",
    "#        with tf.GradientTape() as g_tape:\n",
    "#            g_loss = g_loss_fn(generator, discriminator, batch_zz, is_training)\n",
    "#        g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "#        g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    for epoch in range(epochs):\n",
    "        batch_z = get_random_z(z_dim, batch_size)\n",
    "        batch_x = next(db_iter)\n",
    "        \n",
    "        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
    "            d_loss, gp = d_loss_gp_fn(generator, discriminator, batch_z, batch_x, l, is_training)\n",
    "            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "        d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "        g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss), 'gp:', float(gp))\n",
    "            z = tf.random.normal([100, z_dim])\n",
    "            fake_image = generator(z, training=False)\n",
    "            img_path = os.path.join('gan_images', 'dc-wgan-gp-%d.png'%epoch)\n",
    "            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
