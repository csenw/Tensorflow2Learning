{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice of CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dev20200315\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            multiple                  884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1052672   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  40970     \n",
      "=================================================================\n",
      "Total params: 21,623,178\n",
      "Trainable params: 21,622,666\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "0 0 loss: 2.3025922775268555\n",
      "0 100 loss: 1.9680006504058838\n",
      "0 200 loss: 1.7205886840820312\n",
      "0 300 loss: 1.7264900207519531\n",
      "0 acc: 0.3602\n",
      "1 0 loss: 1.5786104202270508\n",
      "1 100 loss: 1.597585916519165\n",
      "1 200 loss: 1.5661921501159668\n",
      "1 300 loss: 1.5583127737045288\n",
      "1 acc: 0.4264\n",
      "2 0 loss: 1.4487497806549072\n",
      "2 100 loss: 1.5209516286849976\n",
      "2 200 loss: 1.2905797958374023\n",
      "2 300 loss: 1.3757078647613525\n",
      "2 acc: 0.4821\n",
      "3 0 loss: 1.3147964477539062\n",
      "3 100 loss: 1.343497395515442\n",
      "3 200 loss: 1.2105414867401123\n",
      "3 300 loss: 1.3797760009765625\n",
      "3 acc: 0.5228\n",
      "4 0 loss: 1.1610419750213623\n",
      "4 100 loss: 1.269972562789917\n",
      "4 200 loss: 1.1690484285354614\n",
      "4 300 loss: 1.213078498840332\n",
      "4 acc: 0.5428\n",
      "5 0 loss: 1.1505292654037476\n",
      "5 100 loss: 1.2710118293762207\n",
      "5 200 loss: 1.0305373668670654\n",
      "5 300 loss: 1.0928843021392822\n",
      "5 acc: 0.5349\n",
      "6 0 loss: 1.157522439956665\n",
      "6 100 loss: 1.324366807937622\n",
      "6 200 loss: 0.9313680529594421\n",
      "6 300 loss: 1.0649149417877197\n",
      "6 acc: 0.5465\n",
      "7 0 loss: 1.117680311203003\n",
      "7 100 loss: 1.0211964845657349\n",
      "7 200 loss: 0.9231129884719849\n",
      "7 300 loss: 1.097145676612854\n",
      "7 acc: 0.5713\n",
      "8 0 loss: 0.9674942493438721\n",
      "8 100 loss: 1.049027681350708\n",
      "8 200 loss: 0.7742800712585449\n",
      "8 300 loss: 0.9569805264472961\n",
      "8 acc: 0.5738\n",
      "9 0 loss: 0.9918351769447327\n",
      "9 100 loss: 0.9405324459075928\n",
      "9 200 loss: 0.8247476816177368\n",
      "9 300 loss: 1.044296145439148\n",
      "9 acc: 0.5958\n",
      "10 0 loss: 0.8214929103851318\n",
      "10 100 loss: 0.9199468493461609\n",
      "10 200 loss: 0.8227511644363403\n",
      "10 300 loss: 0.7495518922805786\n",
      "10 acc: 0.6083\n",
      "11 0 loss: 0.852422297000885\n",
      "11 100 loss: 1.078870177268982\n",
      "11 200 loss: 0.7887727618217468\n",
      "11 300 loss: 0.8675016164779663\n",
      "11 acc: 0.611\n",
      "12 0 loss: 0.7583773136138916\n",
      "12 100 loss: 0.8219260573387146\n",
      "12 200 loss: 0.7126006484031677\n",
      "12 300 loss: 0.7930750250816345\n",
      "12 acc: 0.6184\n",
      "13 0 loss: 0.6188327074050903\n",
      "13 100 loss: 0.8293726444244385\n",
      "13 200 loss: 0.636049747467041\n",
      "13 300 loss: 0.8181185722351074\n",
      "13 acc: 0.6055\n",
      "14 0 loss: 0.6427609920501709\n",
      "14 100 loss: 0.7284255027770996\n",
      "14 200 loss: 0.7378569841384888\n",
      "14 300 loss: 0.9265351295471191\n",
      "14 acc: 0.6128\n",
      "15 0 loss: 0.5110716819763184\n",
      "15 100 loss: 0.637847900390625\n",
      "15 200 loss: 0.5324100852012634\n",
      "15 300 loss: 0.556717038154602\n",
      "15 acc: 0.6205\n",
      "16 0 loss: 0.445939302444458\n",
      "16 100 loss: 0.6655322909355164\n",
      "16 200 loss: 0.49572086334228516\n",
      "16 300 loss: 0.5988106727600098\n",
      "16 acc: 0.6064\n",
      "17 0 loss: 0.5883504152297974\n",
      "17 100 loss: 0.723156213760376\n",
      "17 200 loss: 0.559984028339386\n",
      "17 300 loss: 0.6731078028678894\n",
      "17 acc: 0.6205\n",
      "18 0 loss: 0.6476216912269592\n",
      "18 100 loss: 0.7172043323516846\n",
      "18 200 loss: 0.6312412023544312\n",
      "18 300 loss: 0.49034246802330017\n",
      "18 acc: 0.6259\n",
      "19 0 loss: 0.5309300422668457\n",
      "19 100 loss: 0.5732477903366089\n",
      "19 200 loss: 0.40982586145401\n",
      "19 300 loss: 0.38805484771728516\n",
      "19 acc: 0.6281\n",
      "20 0 loss: 0.387692928314209\n",
      "20 100 loss: 0.4256671369075775\n",
      "20 200 loss: 0.4170936942100525\n",
      "20 300 loss: 0.5114636421203613\n",
      "20 acc: 0.6287\n",
      "21 0 loss: 0.40173110365867615\n",
      "21 100 loss: 0.6846754550933838\n",
      "21 200 loss: 0.33049553632736206\n",
      "21 300 loss: 0.3225935697555542\n",
      "21 acc: 0.6217\n",
      "22 0 loss: 0.3645092248916626\n",
      "22 100 loss: 0.6190285086631775\n",
      "22 200 loss: 0.40974152088165283\n",
      "22 300 loss: 0.50871342420578\n",
      "22 acc: 0.6188\n",
      "23 0 loss: 0.2802264392375946\n",
      "23 100 loss: 0.3434489667415619\n",
      "23 200 loss: 0.3816509246826172\n",
      "23 300 loss: 0.37343305349349976\n",
      "23 acc: 0.6289\n",
      "24 0 loss: 0.22976166009902954\n",
      "24 100 loss: 0.31394219398498535\n",
      "24 200 loss: 0.3275395631790161\n",
      "24 300 loss: 0.44768059253692627\n",
      "24 acc: 0.6257\n",
      "25 0 loss: 0.38293588161468506\n",
      "25 100 loss: 0.43971627950668335\n",
      "25 200 loss: 0.4184337854385376\n",
      "25 300 loss: 0.27669212222099304\n",
      "25 acc: 0.6207\n",
      "26 0 loss: 0.19736038148403168\n",
      "26 100 loss: 0.31490272283554077\n",
      "26 200 loss: 0.31129106879234314\n",
      "26 300 loss: 0.25932642817497253\n",
      "26 acc: 0.6293\n",
      "27 0 loss: 0.2719551920890808\n",
      "27 100 loss: 0.38129815459251404\n",
      "27 200 loss: 0.47593528032302856\n",
      "27 300 loss: 0.2360163927078247\n",
      "27 acc: 0.6235\n",
      "28 0 loss: 0.20388390123844147\n",
      "28 100 loss: 0.28624942898750305\n",
      "28 200 loss: 0.20911476016044617\n",
      "28 300 loss: 0.1938869059085846\n",
      "28 acc: 0.6259\n",
      "29 0 loss: 0.12720759212970734\n",
      "29 100 loss: 0.14426982402801514\n",
      "29 200 loss: 0.338834673166275\n",
      "29 300 loss: 0.15703484416007996\n",
      "29 acc: 0.6227\n",
      "30 0 loss: 0.18215376138687134\n",
      "30 100 loss: 0.19411221146583557\n",
      "30 200 loss: 0.2865453064441681\n",
      "30 300 loss: 0.25824612379074097\n",
      "30 acc: 0.6206\n",
      "31 0 loss: 0.17212072014808655\n",
      "31 100 loss: 0.29827842116355896\n",
      "31 200 loss: 0.23864246904850006\n",
      "31 300 loss: 0.15882331132888794\n",
      "31 acc: 0.6349\n",
      "32 0 loss: 0.17229577898979187\n",
      "32 100 loss: 0.16025692224502563\n",
      "32 200 loss: 0.252626895904541\n",
      "32 300 loss: 0.1361197531223297\n",
      "32 acc: 0.6287\n",
      "33 0 loss: 0.14500640332698822\n",
      "33 100 loss: 0.1809719204902649\n",
      "33 200 loss: 0.10240119695663452\n",
      "33 300 loss: 0.15902118384838104\n",
      "33 acc: 0.6219\n",
      "34 0 loss: 0.14856992661952972\n",
      "34 100 loss: 0.16157224774360657\n",
      "34 200 loss: 0.1273033320903778\n",
      "34 300 loss: 0.0866282507777214\n",
      "34 acc: 0.6235\n",
      "35 0 loss: 0.11418917775154114\n",
      "35 100 loss: 0.1826499104499817\n",
      "35 200 loss: 0.13517309725284576\n",
      "35 300 loss: 0.17360800504684448\n",
      "35 acc: 0.6253\n",
      "36 0 loss: 0.1469038873910904\n",
      "36 100 loss: 0.1338571012020111\n",
      "36 200 loss: 0.14146757125854492\n",
      "36 300 loss: 0.1074230745434761\n",
      "36 acc: 0.623\n",
      "37 0 loss: 0.32562965154647827\n",
      "37 100 loss: 0.09361262619495392\n",
      "37 200 loss: 0.09689158201217651\n",
      "37 300 loss: 0.08846567571163177\n",
      "37 acc: 0.6208\n",
      "38 0 loss: 0.07758788764476776\n",
      "38 100 loss: 0.19762039184570312\n",
      "38 200 loss: 0.11140073835849762\n",
      "38 300 loss: 0.16245335340499878\n",
      "38 acc: 0.6272\n",
      "39 0 loss: 0.1190062016248703\n",
      "39 100 loss: 0.19693660736083984\n",
      "39 200 loss: 0.10360519587993622\n",
      "39 300 loss: 0.12564852833747864\n",
      "39 acc: 0.6276\n",
      "40 0 loss: 0.05504443868994713\n",
      "40 100 loss: 0.1334955096244812\n",
      "40 200 loss: 0.13443389534950256\n",
      "40 300 loss: 0.16564691066741943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 acc: 0.6212\n",
      "41 0 loss: 0.0911550298333168\n",
      "41 100 loss: 0.23329493403434753\n",
      "41 200 loss: 0.16413424909114838\n",
      "41 300 loss: 0.09899604320526123\n",
      "41 acc: 0.6316\n",
      "42 0 loss: 0.10320775210857391\n",
      "42 100 loss: 0.18526826798915863\n",
      "42 200 loss: 0.1241070106625557\n",
      "42 300 loss: 0.145357146859169\n",
      "42 acc: 0.6125\n",
      "43 0 loss: 0.3200679123401642\n",
      "43 100 loss: 0.1867547333240509\n",
      "43 200 loss: 0.05369972437620163\n",
      "43 300 loss: 0.1887812614440918\n",
      "43 acc: 0.6203\n",
      "44 0 loss: 0.12022589892148972\n",
      "44 100 loss: 0.108853779733181\n",
      "44 200 loss: 0.06529562920331955\n",
      "44 300 loss: 0.11896373331546783\n",
      "44 acc: 0.6298\n",
      "45 0 loss: 0.11959882080554962\n",
      "45 100 loss: 0.08333136886358261\n",
      "45 200 loss: 0.12136460840702057\n",
      "45 300 loss: 0.14014284312725067\n",
      "45 acc: 0.6292\n",
      "46 0 loss: 0.09555593132972717\n",
      "46 100 loss: 0.07337336242198944\n",
      "46 200 loss: 0.08251248300075531\n",
      "46 300 loss: 0.1023854911327362\n",
      "46 acc: 0.6348\n",
      "47 0 loss: 0.046504076570272446\n",
      "47 100 loss: 0.10535635054111481\n",
      "47 200 loss: 0.1755157709121704\n",
      "47 300 loss: 0.04838006943464279\n",
      "47 acc: 0.6364\n",
      "48 0 loss: 0.1341554969549179\n",
      "48 100 loss: 0.08402606099843979\n",
      "48 200 loss: 0.09396255761384964\n",
      "48 300 loss: 0.07827761769294739\n",
      "48 acc: 0.6224\n",
      "49 0 loss: 0.10263535380363464\n",
      "49 100 loss: 0.13270297646522522\n",
      "49 200 loss: 0.115753173828125\n",
      "49 300 loss: 0.05276978388428688\n",
      "49 acc: 0.6264\n",
      "50 0 loss: 0.05608519911766052\n",
      "50 100 loss: 0.06714122742414474\n",
      "50 200 loss: 0.10443457961082458\n",
      "50 300 loss: 0.04702126234769821\n",
      "50 acc: 0.615\n",
      "51 0 loss: 0.09114953130483627\n",
      "51 100 loss: 0.3469463288784027\n",
      "51 200 loss: 0.053601253777742386\n",
      "51 300 loss: 0.030802495777606964\n",
      "51 acc: 0.6353\n",
      "52 0 loss: 0.010322971269488335\n",
      "52 100 loss: 0.027511082589626312\n",
      "52 200 loss: 0.14030170440673828\n",
      "52 300 loss: 0.04414820671081543\n",
      "52 acc: 0.6287\n",
      "53 0 loss: 0.040922604501247406\n",
      "53 100 loss: 0.175567626953125\n",
      "53 200 loss: 0.13588863611221313\n",
      "53 300 loss: 0.0742637887597084\n",
      "53 acc: 0.6328\n",
      "54 0 loss: 0.09338447451591492\n",
      "54 100 loss: 0.022531770169734955\n",
      "54 200 loss: 0.0507994145154953\n",
      "54 300 loss: 0.10796098411083221\n",
      "54 acc: 0.6296\n",
      "55 0 loss: 0.057266220450401306\n",
      "55 100 loss: 0.0943869799375534\n",
      "55 200 loss: 0.09486249089241028\n",
      "55 300 loss: 0.10206678509712219\n",
      "55 acc: 0.6316\n",
      "56 0 loss: 0.0790722519159317\n",
      "56 100 loss: 0.06727119535207748\n",
      "56 200 loss: 0.06820308417081833\n",
      "56 300 loss: 0.03207454830408096\n",
      "56 acc: 0.635\n",
      "57 0 loss: 0.06083283573389053\n",
      "57 100 loss: 0.05941546708345413\n",
      "57 200 loss: 0.020877279341220856\n",
      "57 300 loss: 0.0637555718421936\n",
      "57 acc: 0.6259\n",
      "58 0 loss: 0.041007041931152344\n",
      "58 100 loss: 0.10670369118452072\n",
      "58 200 loss: 0.1617538183927536\n",
      "58 300 loss: 0.07684817910194397\n",
      "58 acc: 0.6377\n",
      "59 0 loss: 0.1215858981013298\n",
      "59 100 loss: 0.07837393879890442\n",
      "59 200 loss: 0.06252025812864304\n",
      "59 300 loss: 0.055466391146183014\n",
      "59 acc: 0.6263\n",
      "60 0 loss: 0.18117119371891022\n",
      "60 100 loss: 0.02351468615233898\n",
      "60 200 loss: 0.1397067755460739\n",
      "60 300 loss: 0.04833074286580086\n",
      "60 acc: 0.6196\n",
      "61 0 loss: 0.14542730152606964\n",
      "61 100 loss: 0.04506922513246536\n",
      "61 200 loss: 0.039593860507011414\n",
      "61 300 loss: 0.020735785365104675\n",
      "61 acc: 0.6334\n",
      "62 0 loss: 0.08805980533361435\n",
      "62 100 loss: 0.0957493782043457\n",
      "62 200 loss: 0.03102010488510132\n",
      "62 300 loss: 0.07740597426891327\n",
      "62 acc: 0.6346\n",
      "63 0 loss: 0.03921616077423096\n",
      "63 100 loss: 0.043013423681259155\n",
      "63 200 loss: 0.16213330626487732\n",
      "63 300 loss: 0.034223224967718124\n",
      "63 acc: 0.6279\n",
      "64 0 loss: 0.05903994292020798\n",
      "64 100 loss: 0.01729174330830574\n",
      "64 200 loss: 0.18455377221107483\n",
      "64 300 loss: 0.014607181772589684\n",
      "64 acc: 0.6291\n",
      "65 0 loss: 0.048678718507289886\n",
      "65 100 loss: 0.04488326609134674\n",
      "65 200 loss: 0.06521812081336975\n",
      "65 300 loss: 0.023767944425344467\n",
      "65 acc: 0.612\n",
      "66 0 loss: 0.11210671812295914\n",
      "66 100 loss: 0.1643570512533188\n",
      "66 200 loss: 0.026476912200450897\n",
      "66 300 loss: 0.025715138763189316\n",
      "66 acc: 0.6314\n",
      "67 0 loss: 0.013761325739324093\n",
      "67 100 loss: 0.04336051270365715\n",
      "67 200 loss: 0.0531478226184845\n",
      "67 300 loss: 0.04486910253763199\n",
      "67 acc: 0.6332\n",
      "68 0 loss: 0.041375841945409775\n",
      "68 100 loss: 0.04436126723885536\n",
      "68 200 loss: 0.05725833773612976\n",
      "68 300 loss: 0.06772235780954361\n",
      "68 acc: 0.6358\n",
      "69 0 loss: 0.020273853093385696\n",
      "69 100 loss: 0.061184994876384735\n",
      "69 200 loss: 0.05504142493009567\n",
      "69 300 loss: 0.00980497244745493\n",
      "69 acc: 0.6311\n",
      "70 0 loss: 0.057907044887542725\n",
      "70 100 loss: 0.09140336513519287\n",
      "70 200 loss: 0.0725163072347641\n",
      "70 300 loss: 0.018338993191719055\n",
      "70 acc: 0.6359\n",
      "71 0 loss: 0.04217416048049927\n",
      "71 100 loss: 0.013219596818089485\n",
      "71 200 loss: 0.11740358173847198\n",
      "71 300 loss: 0.059644147753715515\n",
      "71 acc: 0.6418\n",
      "72 0 loss: 0.053734250366687775\n",
      "72 100 loss: 0.03683748468756676\n",
      "72 200 loss: 0.03850160166621208\n",
      "72 300 loss: 0.0719163566827774\n",
      "72 acc: 0.6406\n",
      "73 0 loss: 0.01698322407901287\n",
      "73 100 loss: 0.15314225852489471\n",
      "73 200 loss: 0.09347212314605713\n",
      "73 300 loss: 0.06285418570041656\n",
      "73 acc: 0.6448\n",
      "74 0 loss: 0.0903884619474411\n",
      "74 100 loss: 0.0267396941781044\n",
      "74 200 loss: 0.028288956731557846\n",
      "74 300 loss: 0.020002366974949837\n",
      "74 acc: 0.6401\n",
      "75 0 loss: 0.08466492593288422\n",
      "75 100 loss: 0.06412841379642487\n",
      "75 200 loss: 0.09313085675239563\n",
      "75 300 loss: 0.06045588478446007\n",
      "75 acc: 0.6308\n",
      "76 0 loss: 0.03460194543004036\n",
      "76 100 loss: 0.06992577761411667\n",
      "76 200 loss: 0.013686011545360088\n",
      "76 300 loss: 0.08689746260643005\n",
      "76 acc: 0.6352\n",
      "77 0 loss: 0.09137748181819916\n",
      "77 100 loss: 0.03773578628897667\n",
      "77 200 loss: 0.03929881751537323\n",
      "77 300 loss: 0.05823704972863197\n",
      "77 acc: 0.626\n",
      "78 0 loss: 0.32374081015586853\n",
      "78 100 loss: 0.06612210720777512\n",
      "78 200 loss: 0.02800995483994484\n",
      "78 300 loss: 0.05551377311348915\n",
      "78 acc: 0.6322\n",
      "79 0 loss: 0.039189983159303665\n",
      "79 100 loss: 0.01256868988275528\n",
      "79 200 loss: 0.05356913432478905\n",
      "79 300 loss: 0.006054650992155075\n",
      "79 acc: 0.6344\n",
      "80 0 loss: 0.039310675114393234\n",
      "80 100 loss: 0.06009429320693016\n",
      "80 200 loss: 0.018926557153463364\n",
      "80 300 loss: 0.12457583844661713\n",
      "80 acc: 0.6397\n",
      "81 0 loss: 0.03894795477390289\n",
      "81 100 loss: 0.04857217147946358\n",
      "81 200 loss: 0.018114596605300903\n",
      "81 300 loss: 0.060424987226724625\n",
      "81 acc: 0.626\n",
      "82 0 loss: 0.07851269841194153\n",
      "82 100 loss: 0.0685788169503212\n",
      "82 200 loss: 0.005425849463790655\n",
      "82 300 loss: 0.031455621123313904\n",
      "82 acc: 0.6441\n",
      "83 0 loss: 0.01584753952920437\n",
      "83 100 loss: 0.011075621470808983\n",
      "83 200 loss: 0.05847291648387909\n",
      "83 300 loss: 0.018438149243593216\n",
      "83 acc: 0.6413\n",
      "84 0 loss: 0.044552575796842575\n",
      "84 100 loss: 0.010291000828146935\n",
      "84 200 loss: 0.03253346309065819\n",
      "84 300 loss: 0.02176462858915329\n",
      "84 acc: 0.6322\n",
      "85 0 loss: 0.02706959657371044\n",
      "85 100 loss: 0.043572843074798584\n",
      "85 200 loss: 0.12379763275384903\n",
      "85 300 loss: 0.10967068374156952\n",
      "85 acc: 0.6406\n",
      "86 0 loss: 0.058787137269973755\n",
      "86 100 loss: 0.013035820797085762\n",
      "86 200 loss: 0.008015934377908707\n",
      "86 300 loss: 0.03103487379848957\n",
      "86 acc: 0.6382\n",
      "87 0 loss: 0.00635414756834507\n",
      "87 100 loss: 0.13940457999706268\n",
      "87 200 loss: 0.01259105559438467\n",
      "87 300 loss: 0.08738663047552109\n",
      "87 acc: 0.6397\n",
      "88 0 loss: 0.055471811443567276\n",
      "88 100 loss: 0.04919448867440224\n",
      "88 200 loss: 0.05921849235892296\n",
      "88 300 loss: 0.034290604293346405\n",
      "88 acc: 0.6294\n",
      "89 0 loss: 0.1772085726261139\n",
      "89 100 loss: 0.02918337658047676\n",
      "89 200 loss: 0.06635983288288116\n",
      "89 300 loss: 0.04228248819708824\n",
      "89 acc: 0.626\n",
      "90 0 loss: 0.05712352320551872\n",
      "90 100 loss: 0.015909675508737564\n",
      "90 200 loss: 0.023262329399585724\n",
      "90 300 loss: 0.01785307191312313\n",
      "90 acc: 0.6339\n",
      "91 0 loss: 0.05889356881380081\n",
      "91 100 loss: 0.08877762407064438\n",
      "91 200 loss: 0.03330579400062561\n",
      "91 300 loss: 0.051400162279605865\n",
      "91 acc: 0.6279\n",
      "92 0 loss: 0.01894264481961727\n",
      "92 100 loss: 0.015925616025924683\n",
      "92 200 loss: 0.022117946296930313\n",
      "92 300 loss: 0.0020009749568998814\n",
      "92 acc: 0.6384\n",
      "93 0 loss: 0.02883114665746689\n",
      "93 100 loss: 0.01695624552667141\n",
      "93 200 loss: 0.014614380896091461\n",
      "93 300 loss: 0.03967136889696121\n",
      "93 acc: 0.6335\n",
      "94 0 loss: 0.03200514242053032\n",
      "94 100 loss: 0.022995073348283768\n",
      "94 200 loss: 0.02645181678235531\n",
      "94 300 loss: 0.14945954084396362\n",
      "94 acc: 0.6417\n",
      "95 0 loss: 0.002567066578194499\n",
      "95 100 loss: 0.044013526290655136\n",
      "95 200 loss: 0.04634047672152519\n",
      "95 300 loss: 0.06945949792861938\n",
      "95 acc: 0.6392\n",
      "96 0 loss: 0.04054585099220276\n",
      "96 100 loss: 0.07667848467826843\n",
      "96 200 loss: 0.020309656858444214\n",
      "96 300 loss: 0.026113325729966164\n",
      "96 acc: 0.6375\n",
      "97 0 loss: 0.017311977222561836\n",
      "97 100 loss: 0.03754885494709015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 200 loss: 0.04609982669353485\n",
      "97 300 loss: 0.015556344762444496\n",
      "97 acc: 0.6498\n",
      "98 0 loss: 0.017875108867883682\n",
      "98 100 loss: 0.05054555460810661\n",
      "98 200 loss: 0.02623177506029606\n",
      "98 300 loss: 0.04815909266471863\n",
      "98 acc: 0.6397\n",
      "99 0 loss: 0.02392824925482273\n",
      "99 100 loss: 0.001988174393773079\n",
      "99 200 loss: 0.09830549359321594\n",
      "99 300 loss: 0.005301022902131081\n",
      "99 acc: 0.6386\n",
      "Iteration finished!!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential\n",
    "import os\n",
    "\n",
    "\n",
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# remove the last division of y and y_test\n",
    "y = tf.squeeze(y, axis=-1)\n",
    "y_test = tf.squeeze(y_test, axis=-1)\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "def preprocess(x,y):\n",
    "    x = 2*tf.cast(x,dtype=tf.float32)/255.-1\n",
    "    y = tf.cast(y,dtype=tf.int32)\n",
    "    return x,y\n",
    "    \n",
    "# need to construct Training object with the imported tensors\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "# shuffle and preporcess and setup batch\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(128)\n",
    "\n",
    "alexnet_layers = [\n",
    "    # Layer 1\n",
    "    layers.Conv2D(filters = 96, kernel_size = [11,11], strides = 4, padding = [[0,0],[2,2],[2,2],[0,0]], activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[3,3], strides=2, padding = 'valid'),\n",
    "    # Layer 2\n",
    "    layers.Conv2D(filters = 256, kernel_size = [5,5], strides = 1, padding = [[0,0],[2,2],[2,2],[0,0]], activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [3,3], strides = 2, padding = 'valid'),\n",
    "    # Layer 3\n",
    "    layers.Conv2D(filters = 384, kernel_size = [3,3], strides = 1, padding = [[0,0],[1,1],[1,1],[0,0]], activation=tf.nn.relu),\n",
    "    # Layer 4\n",
    "    layers.Conv2D(filters = 384, kernel_size = [3,3], strides = 1, padding = [[0,0],[1,1],[1,1],[0,0]], activation=tf.nn.relu),\n",
    "    # Layer 5\n",
    "    layers.Conv2D(filters = 256, kernel_size = [3,3], strides = 1, padding = [[0,0],[1,1],[1,1],[0,0]], activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [3,3], strides=1, padding = 'same'),\n",
    "    \n",
    "    # Before going to FC layers, a batch normal layer is needed.\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # FC layers\n",
    "    # Layer 6\n",
    "    layers.Dense(4096, activation = tf.nn.relu),\n",
    "    layers.Dropout(rate = 0.2),\n",
    "    # Layer 7\n",
    "    layers.Dense(4096, activation = tf.nn.relu),\n",
    "    layers.Dropout(rate = 0.2),\n",
    "    # Layer 8\n",
    "    layers.Dense(10, activation=None)\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    alexnet = Sequential(alexnet_layers)\n",
    "    alexnet.build(input_shape = [None, 32, 32, 3])\n",
    "    alexnet.summary()\n",
    "    #vgg_layers = []\n",
    "    \n",
    "    optimizer = optimizers.Adam(lr = 1e-4)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = alexnet(x)\n",
    "                y_onehot = tf.one_hot(y, depth = 10)\n",
    "\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, out, from_logits = True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, alexnet.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, alexnet.trainable_variables))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss:', float(loss))\n",
    "\n",
    "        total_num = 0\n",
    "        total_correct = 0\n",
    "        for x,y in test_db:\n",
    "            out = alexnet(x)\n",
    "            prob = tf.nn.softmax(out, axis = 1)\n",
    "            pred = tf.argmax(prob, axis = 1)\n",
    "            pred = tf.cast(pred, dtype = tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "\n",
    "            total_num += x.shape[0]\n",
    "            total_correct += int(correct)\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'acc:', acc)\n",
    "      \n",
    "    print(\"Iteration finished!!\")\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 11553341440\n",
      "Free memory: 11046354944\n",
      "Used memory: 506986496\n"
     ]
    }
   ],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
