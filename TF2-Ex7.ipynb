{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Libs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dev20200315\n"
     ]
    }
   ],
   "source": [
    "# All required libs are imporeted in this cell\n",
    "import os, glob, multiprocessing\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, ReLU, LeakyReLU, Activation\n",
    "from tensorflow.keras import Sequential, optimizers\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "z_dim = 100\n",
    "epochs = 3000000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "is_training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Functions/Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Files: 51223\n",
      "<PrefetchDataset shapes: (128, 64, 64, 3), types: tf.float32> (64, 64, 3)\n",
      "(128, 64, 64, 3) 1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n",
    "\n",
    "    # @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.image.resize(img, [resize, resize])\n",
    "        # img = tf.image.random_crop(img,[resize, resize])\n",
    "        # img = tf.image.random_flip_left_right(img)\n",
    "        # img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.clip_by_value(img, 0, 255)\n",
    "        img = img / 127.5 - 1 #-1~1\n",
    "        return img\n",
    "\n",
    "    dataset = disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) // batch_size\n",
    "\n",
    "    return dataset, img_shape, len_dataset\n",
    "\n",
    "\n",
    "def batch_dataset(dataset,\n",
    "                  batch_size,\n",
    "                  drop_remainder=True,\n",
    "                  n_prefetch_batch=1,\n",
    "                  filter_fn=None,\n",
    "                  map_fn=None,\n",
    "                  n_map_threads=None,\n",
    "                  filter_after_map=False,\n",
    "                  shuffle=True,\n",
    "                  shuffle_buffer_size=None,\n",
    "                  repeat=None):\n",
    "    # set defaults\n",
    "    if n_map_threads is None:\n",
    "        n_map_threads = multiprocessing.cpu_count()\n",
    "    if shuffle and shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
    "\n",
    "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if not filter_after_map:\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "    else:  # [*] this is slower\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def memory_data_batch_dataset(memory_data,\n",
    "                              batch_size,\n",
    "                              drop_remainder=True,\n",
    "                              n_prefetch_batch=1,\n",
    "                              filter_fn=None,\n",
    "                              map_fn=None,\n",
    "                              n_map_threads=None,\n",
    "                              filter_after_map=False,\n",
    "                              shuffle=True,\n",
    "                              shuffle_buffer_size=None,\n",
    "                              repeat=None):\n",
    "    \"\"\"Batch dataset of memory data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_data : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
    "    dataset = batch_dataset(dataset,\n",
    "                            batch_size,\n",
    "                            drop_remainder=drop_remainder,\n",
    "                            n_prefetch_batch=n_prefetch_batch,\n",
    "                            filter_fn=filter_fn,\n",
    "                            map_fn=map_fn,\n",
    "                            n_map_threads=n_map_threads,\n",
    "                            filter_after_map=filter_after_map,\n",
    "                            shuffle=shuffle,\n",
    "                            shuffle_buffer_size=shuffle_buffer_size,\n",
    "                            repeat=repeat)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def disk_image_batch_dataset(img_paths,\n",
    "                             batch_size,\n",
    "                             labels=None,\n",
    "                             drop_remainder=True,\n",
    "                             n_prefetch_batch=1,\n",
    "                             filter_fn=None,\n",
    "                             map_fn=None,\n",
    "                             n_map_threads=None,\n",
    "                             filter_after_map=False,\n",
    "                             shuffle=True,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             repeat=None):\n",
    "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_paths : 1d-tensor/ndarray/list of str\n",
    "        labels : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        memory_data = img_paths\n",
    "    else:\n",
    "        memory_data = (img_paths, labels)\n",
    "\n",
    "    def parse_fn(path, *label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n",
    "        return (img,) + label\n",
    "\n",
    "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
    "        def map_fn_(*args):\n",
    "            return map_fn(*parse_fn(*args))\n",
    "    else:\n",
    "        map_fn_ = parse_fn\n",
    "\n",
    "    dataset = memory_data_batch_dataset(memory_data,\n",
    "                                        batch_size,\n",
    "                                        drop_remainder=drop_remainder,\n",
    "                                        n_prefetch_batch=n_prefetch_batch,\n",
    "                                        filter_fn=filter_fn,\n",
    "                                        map_fn=map_fn_,\n",
    "                                        n_map_threads=n_map_threads,\n",
    "                                        filter_after_map=filter_after_map,\n",
    "                                        shuffle=shuffle,\n",
    "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                        repeat=repeat)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_random_z(z_dim, batch_size):\n",
    "    return tf.random.uniform([batch_size, z_dim], minval=-1, maxval=1)\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    y = tf.ones_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits = True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    y = tf.zeros_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits = True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Loss function for Discriminator\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    #print(fake_image)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    \n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    \n",
    "    return d_loss_fake + d_loss_real\n",
    "\n",
    "def d_loss_fn2(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    #print(fake_image)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    \n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    \n",
    "    return d_loss_fake, d_loss_real, 0.5*d_loss_fake + 0.5*d_loss_real\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    return celoss_ones(d_fake_logits)\n",
    "\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    #toimage(final_image).save(image_path)\n",
    "    #print(image_path)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "    \n",
    "img_path = glob.glob('./faces/*.jpg')\n",
    "print(\"Num of Files:\",len(img_path))\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize = 64)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset))\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(), tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat(100)\n",
    "db_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definintions and Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubClass Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 D-loss: 1.3412562608718872 G-loss: 6.58075475692749\n",
      "Epoch: 100 D-loss: 1.1215951442718506 G-loss: 4.165923118591309\n",
      "Epoch: 200 D-loss: 0.9397129416465759 G-loss: 6.437928199768066\n",
      "Epoch: 300 D-loss: 0.768599808216095 G-loss: 5.651960372924805\n",
      "Epoch: 400 D-loss: 0.6186394095420837 G-loss: 7.133687496185303\n",
      "Epoch: 500 D-loss: 0.8950461149215698 G-loss: 2.645596504211426\n",
      "Epoch: 600 D-loss: 0.8223548531532288 G-loss: 1.897244930267334\n",
      "Epoch: 700 D-loss: 0.428438663482666 G-loss: 2.509617328643799\n",
      "Epoch: 800 D-loss: 0.6410261988639832 G-loss: 3.698641300201416\n",
      "Epoch: 900 D-loss: 0.43173879384994507 G-loss: 4.962777137756348\n",
      "Epoch: 1000 D-loss: 0.590599775314331 G-loss: 4.430183410644531\n",
      "Epoch: 1100 D-loss: 0.5221167802810669 G-loss: 4.676104545593262\n",
      "Epoch: 1200 D-loss: 0.8785750865936279 G-loss: 6.312414169311523\n",
      "Epoch: 1300 D-loss: 0.32847410440444946 G-loss: 4.591730117797852\n",
      "Epoch: 1400 D-loss: 0.6811486482620239 G-loss: 4.339748382568359\n",
      "Epoch: 1500 D-loss: 0.4471210241317749 G-loss: 2.7007288932800293\n",
      "Epoch: 1600 D-loss: 0.7094177007675171 G-loss: 4.607505798339844\n",
      "Epoch: 1700 D-loss: 0.7093408107757568 G-loss: 3.534179210662842\n",
      "Epoch: 1800 D-loss: 0.7743406891822815 G-loss: 3.028756618499756\n",
      "Epoch: 1900 D-loss: 0.6037136912345886 G-loss: 3.0204520225524902\n",
      "Epoch: 2000 D-loss: 1.561025857925415 G-loss: 10.705589294433594\n",
      "Epoch: 2100 D-loss: 0.4158123731613159 G-loss: 5.064390659332275\n",
      "Epoch: 2200 D-loss: 0.8190906047821045 G-loss: 3.1312718391418457\n",
      "Epoch: 2300 D-loss: 0.6757510900497437 G-loss: 2.9313149452209473\n",
      "Epoch: 2400 D-loss: 0.4948599338531494 G-loss: 4.487343788146973\n",
      "Epoch: 2500 D-loss: 0.4523674249649048 G-loss: 3.900399923324585\n",
      "Epoch: 2600 D-loss: 0.4913291931152344 G-loss: 4.828313827514648\n",
      "Epoch: 2700 D-loss: 0.32192960381507874 G-loss: 4.392773151397705\n",
      "Epoch: 2800 D-loss: 0.3868602514266968 G-loss: 5.062131881713867\n",
      "Epoch: 2900 D-loss: 0.48330843448638916 G-loss: 3.821335554122925\n",
      "Epoch: 3000 D-loss: 0.3788836598396301 G-loss: 3.194913625717163\n",
      "Epoch: 3100 D-loss: 0.5240947008132935 G-loss: 4.529567241668701\n",
      "Epoch: 3200 D-loss: 0.2829533517360687 G-loss: 4.524360656738281\n",
      "Epoch: 3300 D-loss: 0.4664766788482666 G-loss: 4.6670379638671875\n",
      "Epoch: 3400 D-loss: 0.535000205039978 G-loss: 6.867964744567871\n",
      "Epoch: 3500 D-loss: 0.3356426954269409 G-loss: 5.544881820678711\n",
      "Epoch: 3600 D-loss: 1.5999279022216797 G-loss: 12.767375946044922\n",
      "Epoch: 3700 D-loss: 0.24753114581108093 G-loss: 4.787107467651367\n",
      "Epoch: 3800 D-loss: 0.512984037399292 G-loss: 6.549704551696777\n",
      "Epoch: 3900 D-loss: 0.2863917350769043 G-loss: 5.688528537750244\n",
      "Epoch: 4000 D-loss: 1.1096028089523315 G-loss: 11.011241912841797\n",
      "Epoch: 4100 D-loss: 0.7236000895500183 G-loss: 5.029753684997559\n",
      "Epoch: 4200 D-loss: 0.1556285321712494 G-loss: 4.392606258392334\n",
      "Epoch: 4300 D-loss: 0.4533475637435913 G-loss: 6.564873695373535\n",
      "Epoch: 4400 D-loss: 0.22288155555725098 G-loss: 4.691657543182373\n",
      "Epoch: 4500 D-loss: 0.4478752017021179 G-loss: 6.004560470581055\n",
      "Epoch: 4600 D-loss: 0.20543330907821655 G-loss: 4.1248626708984375\n",
      "Epoch: 4700 D-loss: 0.8094680905342102 G-loss: 8.49251937866211\n",
      "Epoch: 4800 D-loss: 0.4419107735157013 G-loss: 8.453950881958008\n",
      "Epoch: 4900 D-loss: 0.29338425397872925 G-loss: 4.972049713134766\n",
      "Epoch: 5000 D-loss: 0.25165122747421265 G-loss: 4.70743989944458\n",
      "Epoch: 5100 D-loss: 0.3812432587146759 G-loss: 7.725284576416016\n",
      "Epoch: 5200 D-loss: 0.2598479688167572 G-loss: 4.065542221069336\n",
      "Epoch: 5300 D-loss: 0.03299376741051674 G-loss: 6.06201171875\n",
      "Epoch: 5400 D-loss: 0.2394326627254486 G-loss: 4.683529853820801\n",
      "Epoch: 5500 D-loss: 0.3565373420715332 G-loss: 7.718179702758789\n",
      "Epoch: 5600 D-loss: 0.4333886504173279 G-loss: 9.716257095336914\n",
      "Epoch: 5700 D-loss: 0.2192246913909912 G-loss: 5.239407539367676\n",
      "Epoch: 5800 D-loss: 0.5820357203483582 G-loss: 8.371845245361328\n",
      "Epoch: 5900 D-loss: 2.4494969844818115 G-loss: 8.911750793457031\n",
      "Epoch: 6000 D-loss: 0.29946503043174744 G-loss: 6.736496925354004\n",
      "Epoch: 6100 D-loss: 0.2824435234069824 G-loss: 7.175431728363037\n",
      "Epoch: 6200 D-loss: 0.5901143550872803 G-loss: 13.146912574768066\n",
      "Epoch: 6300 D-loss: 0.2915882170200348 G-loss: 6.879315376281738\n",
      "Epoch: 6400 D-loss: 0.08593934774398804 G-loss: 4.879605770111084\n",
      "Epoch: 6500 D-loss: 0.17162536084651947 G-loss: 6.438859939575195\n",
      "Epoch: 6600 D-loss: 0.4320814311504364 G-loss: 6.854742527008057\n",
      "Epoch: 6700 D-loss: 0.3499656915664673 G-loss: 3.675575017929077\n",
      "Epoch: 6800 D-loss: 0.13999825716018677 G-loss: 5.48371696472168\n",
      "Epoch: 6900 D-loss: 0.15659363567829132 G-loss: 6.2023844718933105\n",
      "Epoch: 7000 D-loss: 0.2855391204357147 G-loss: 7.088435173034668\n",
      "Epoch: 7100 D-loss: 0.22151672840118408 G-loss: 5.120009422302246\n",
      "Epoch: 7200 D-loss: 0.28533104062080383 G-loss: 6.096550941467285\n",
      "Epoch: 7300 D-loss: 0.12261991202831268 G-loss: 6.372231483459473\n",
      "Epoch: 7400 D-loss: 0.32414865493774414 G-loss: 9.202220916748047\n",
      "Epoch: 7500 D-loss: 0.3077991008758545 G-loss: 4.497227668762207\n",
      "Epoch: 7600 D-loss: 0.10022775083780289 G-loss: 5.8053388595581055\n",
      "Epoch: 7700 D-loss: 0.20777316391468048 G-loss: 5.271384239196777\n",
      "Epoch: 7800 D-loss: 0.3577147424221039 G-loss: 10.618377685546875\n",
      "Epoch: 7900 D-loss: 0.151133194565773 G-loss: 9.021455764770508\n",
      "Epoch: 8000 D-loss: 0.1440492868423462 G-loss: 5.665167808532715\n",
      "Epoch: 8100 D-loss: 0.18831941485404968 G-loss: 4.698433876037598\n",
      "Epoch: 8200 D-loss: 0.24973571300506592 G-loss: 4.662146091461182\n",
      "Epoch: 8300 D-loss: 0.10970562696456909 G-loss: 5.081946849822998\n",
      "Epoch: 8400 D-loss: 0.10533861070871353 G-loss: 7.653593063354492\n",
      "Epoch: 8500 D-loss: 0.1312287151813507 G-loss: 6.765809059143066\n",
      "Epoch: 8600 D-loss: 0.5475308895111084 G-loss: 14.338264465332031\n",
      "Epoch: 8700 D-loss: 0.41112297773361206 G-loss: 11.833559036254883\n",
      "Epoch: 8800 D-loss: 0.48732590675354004 G-loss: 10.226099967956543\n",
      "Epoch: 8900 D-loss: 0.1532256007194519 G-loss: 3.7079668045043945\n",
      "Epoch: 9000 D-loss: 0.06317602097988129 G-loss: 6.331200122833252\n",
      "Epoch: 9100 D-loss: 0.3561047315597534 G-loss: 10.699777603149414\n",
      "Epoch: 9200 D-loss: 0.455447793006897 G-loss: 15.065007209777832\n",
      "Epoch: 9300 D-loss: 0.13749228417873383 G-loss: 5.886422157287598\n",
      "Epoch: 9400 D-loss: 4.630600929260254 G-loss: 10.161418914794922\n",
      "Epoch: 9500 D-loss: 0.1377628743648529 G-loss: 5.177859306335449\n",
      "Epoch: 9600 D-loss: 0.26242342591285706 G-loss: 4.523303031921387\n",
      "Epoch: 9700 D-loss: 0.18549251556396484 G-loss: 7.075227737426758\n",
      "Epoch: 9800 D-loss: 0.2613217234611511 G-loss: 8.080536842346191\n",
      "Epoch: 9900 D-loss: 0.16852852702140808 G-loss: 5.661081314086914\n",
      "Epoch: 10000 D-loss: 0.18122173845767975 G-loss: 3.774641752243042\n",
      "Epoch: 10100 D-loss: 0.22067269682884216 G-loss: 9.347267150878906\n",
      "Epoch: 10200 D-loss: 0.09085144102573395 G-loss: 7.328121185302734\n",
      "Epoch: 10300 D-loss: 0.1759713590145111 G-loss: 6.254160404205322\n",
      "Epoch: 10400 D-loss: 0.07326649129390717 G-loss: 7.031730651855469\n",
      "Epoch: 10500 D-loss: 0.13482584059238434 G-loss: 6.982930660247803\n",
      "Epoch: 10600 D-loss: 0.06597089022397995 G-loss: 6.5360212326049805\n",
      "Epoch: 10700 D-loss: 0.1865096241235733 G-loss: 4.393552780151367\n",
      "Epoch: 10800 D-loss: 0.11901891231536865 G-loss: 5.64095401763916\n",
      "Epoch: 10900 D-loss: 0.28606316447257996 G-loss: 10.57890796661377\n",
      "Epoch: 11000 D-loss: 0.17101146280765533 G-loss: 7.383544921875\n",
      "Epoch: 11100 D-loss: 0.26253584027290344 G-loss: 8.508834838867188\n",
      "Epoch: 11200 D-loss: 0.1070801317691803 G-loss: 7.8082275390625\n",
      "Epoch: 11300 D-loss: 0.24640750885009766 G-loss: 7.34181547164917\n",
      "Epoch: 11400 D-loss: 0.10163246095180511 G-loss: 4.095524787902832\n",
      "Epoch: 11500 D-loss: 0.20158489048480988 G-loss: 7.4360880851745605\n",
      "Epoch: 11600 D-loss: 0.34457072615623474 G-loss: 8.970529556274414\n",
      "Epoch: 11700 D-loss: 0.16326376795768738 G-loss: 8.333220481872559\n",
      "Epoch: 11800 D-loss: 0.5380008220672607 G-loss: 14.523653030395508\n",
      "Epoch: 11900 D-loss: 0.12267884612083435 G-loss: 6.029172897338867\n",
      "Epoch: 12000 D-loss: 0.09796833992004395 G-loss: 6.775447845458984\n",
      "Epoch: 12100 D-loss: 0.30227014422416687 G-loss: 6.100707530975342\n",
      "Epoch: 12200 D-loss: 0.11567269265651703 G-loss: 6.715463161468506\n",
      "Epoch: 12300 D-loss: 0.3078634738922119 G-loss: 6.211534023284912\n",
      "Epoch: 12400 D-loss: 0.1973971724510193 G-loss: 5.315790176391602\n",
      "Epoch: 12500 D-loss: 0.139174684882164 G-loss: 5.185588836669922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12600 D-loss: 0.27199143171310425 G-loss: 4.879262447357178\n",
      "Epoch: 12700 D-loss: 0.16385731101036072 G-loss: 14.970977783203125\n",
      "Epoch: 12800 D-loss: 0.17875611782073975 G-loss: 3.6928658485412598\n",
      "Epoch: 12900 D-loss: 0.2202681601047516 G-loss: 7.2023725509643555\n",
      "Epoch: 13000 D-loss: 0.13497138023376465 G-loss: 5.979381084442139\n",
      "Epoch: 13100 D-loss: 0.216679647564888 G-loss: 7.364302635192871\n",
      "Epoch: 13200 D-loss: 1.8129794597625732 G-loss: 23.03577423095703\n",
      "Epoch: 13300 D-loss: 0.4887448847293854 G-loss: 8.37378978729248\n",
      "Epoch: 13400 D-loss: 0.22471672296524048 G-loss: 6.948856353759766\n",
      "Epoch: 13500 D-loss: 0.03435755521059036 G-loss: 7.634442329406738\n",
      "Epoch: 13600 D-loss: 0.7663455009460449 G-loss: 15.629377365112305\n",
      "Epoch: 13700 D-loss: 0.3780936002731323 G-loss: 2.7472879886627197\n",
      "Epoch: 13800 D-loss: 0.02636723779141903 G-loss: 9.841804504394531\n",
      "Epoch: 13900 D-loss: 0.1499701589345932 G-loss: 10.093732833862305\n",
      "Epoch: 14000 D-loss: 0.06503552198410034 G-loss: 6.674544811248779\n",
      "Epoch: 14100 D-loss: 0.45256564021110535 G-loss: 6.310152530670166\n",
      "Epoch: 14200 D-loss: 0.4469040632247925 G-loss: 10.009799003601074\n",
      "Epoch: 14300 D-loss: 0.0879020243883133 G-loss: 6.3378705978393555\n",
      "Epoch: 14400 D-loss: 0.14972412586212158 G-loss: 8.695806503295898\n",
      "Epoch: 14500 D-loss: 0.03861234337091446 G-loss: 8.286371231079102\n",
      "Epoch: 14600 D-loss: 0.4426676630973816 G-loss: 13.439220428466797\n",
      "Epoch: 14700 D-loss: 0.23525577783584595 G-loss: 10.368393898010254\n",
      "Epoch: 14800 D-loss: 0.28363293409347534 G-loss: 8.212154388427734\n",
      "Epoch: 14900 D-loss: 0.13003510236740112 G-loss: 6.641938209533691\n",
      "Epoch: 15000 D-loss: 0.07589249312877655 G-loss: 14.966547012329102\n",
      "Epoch: 15100 D-loss: 0.14208850264549255 G-loss: 8.497068405151367\n",
      "Epoch: 15200 D-loss: 0.11283335089683533 G-loss: 7.475735187530518\n",
      "Epoch: 15300 D-loss: 0.5228006839752197 G-loss: 7.057847023010254\n",
      "Epoch: 15400 D-loss: 0.19374023377895355 G-loss: 5.026763916015625\n",
      "Epoch: 15500 D-loss: 0.13345515727996826 G-loss: 7.640569686889648\n",
      "Epoch: 15600 D-loss: 0.25540828704833984 G-loss: 11.508529663085938\n",
      "Epoch: 15700 D-loss: 0.06691470742225647 G-loss: 6.659411430358887\n",
      "Epoch: 15800 D-loss: 0.08487152308225632 G-loss: 8.50051498413086\n",
      "Epoch: 15900 D-loss: 0.1027878075838089 G-loss: 6.734701156616211\n",
      "Epoch: 16000 D-loss: 0.042529888451099396 G-loss: 7.49871826171875\n",
      "Epoch: 16100 D-loss: 0.22149421274662018 G-loss: 7.877588272094727\n",
      "Epoch: 16200 D-loss: 0.034300632774829865 G-loss: 5.572014808654785\n",
      "Epoch: 16300 D-loss: 0.18646354973316193 G-loss: 12.291864395141602\n",
      "Epoch: 16400 D-loss: 0.2556734085083008 G-loss: 12.902172088623047\n",
      "Epoch: 16500 D-loss: 0.08146999776363373 G-loss: 5.91864538192749\n",
      "Epoch: 16600 D-loss: 0.13719531893730164 G-loss: 4.615787982940674\n",
      "Epoch: 16700 D-loss: 0.029340609908103943 G-loss: 5.820480823516846\n",
      "Epoch: 16800 D-loss: 0.10902261734008789 G-loss: 7.755100250244141\n",
      "Epoch: 16900 D-loss: 0.41605401039123535 G-loss: 8.898513793945312\n",
      "Epoch: 17000 D-loss: 0.14033228158950806 G-loss: 7.369256973266602\n",
      "Epoch: 17100 D-loss: 0.16003549098968506 G-loss: 8.795896530151367\n",
      "Epoch: 17200 D-loss: 0.0936153382062912 G-loss: 9.30455207824707\n",
      "Epoch: 17300 D-loss: 0.03902285918593407 G-loss: 7.176165580749512\n",
      "Epoch: 17400 D-loss: 0.08955387771129608 G-loss: 5.173696994781494\n",
      "Epoch: 17500 D-loss: 0.1020602285861969 G-loss: 6.564865589141846\n",
      "Epoch: 17600 D-loss: 0.06917412579059601 G-loss: 6.583232879638672\n",
      "Epoch: 17700 D-loss: 0.1651616394519806 G-loss: 5.152320861816406\n",
      "Epoch: 17800 D-loss: 0.11198298633098602 G-loss: 5.1734161376953125\n",
      "Epoch: 17900 D-loss: 0.08100923150777817 G-loss: 5.441105842590332\n",
      "Epoch: 18000 D-loss: 0.016002710908651352 G-loss: 9.845846176147461\n",
      "Epoch: 18100 D-loss: 0.19382941722869873 G-loss: 11.816579818725586\n",
      "Epoch: 18200 D-loss: 0.010874622501432896 G-loss: 7.0193634033203125\n",
      "Epoch: 18300 D-loss: 0.05545554310083389 G-loss: 5.920735836029053\n",
      "Epoch: 18400 D-loss: 0.1593073308467865 G-loss: 13.066316604614258\n",
      "Epoch: 18500 D-loss: 0.0932621955871582 G-loss: 6.600611686706543\n",
      "Epoch: 18600 D-loss: 3.016146421432495 G-loss: 23.41741943359375\n",
      "Epoch: 18700 D-loss: 0.1646524965763092 G-loss: 5.039324760437012\n",
      "Epoch: 18800 D-loss: 0.182490274310112 G-loss: 3.90114688873291\n",
      "Epoch: 18900 D-loss: 0.14907409250736237 G-loss: 6.640538692474365\n",
      "Epoch: 19000 D-loss: 0.3021605908870697 G-loss: 9.97231674194336\n",
      "Epoch: 19100 D-loss: 0.18128715455532074 G-loss: 9.068181991577148\n",
      "Epoch: 19200 D-loss: 0.20837505161762238 G-loss: 6.647128582000732\n",
      "Epoch: 19300 D-loss: 0.1822030395269394 G-loss: 7.12152099609375\n",
      "Epoch: 19400 D-loss: 0.14689695835113525 G-loss: 8.094844818115234\n",
      "Epoch: 19500 D-loss: 0.10403304547071457 G-loss: 6.853324890136719\n",
      "Epoch: 19600 D-loss: 0.05382920056581497 G-loss: 5.810370922088623\n",
      "Epoch: 19700 D-loss: 0.13012678921222687 G-loss: 8.01702880859375\n",
      "Epoch: 19800 D-loss: 0.029121778905391693 G-loss: 7.744374752044678\n",
      "Epoch: 19900 D-loss: 0.19382531940937042 G-loss: 10.64442253112793\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-87e336c11d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discriminator.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-87e336c11d29>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mbatch_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.s = 2\n",
    "        self.k = 4\n",
    "        self.n_f = 1024\n",
    "        \n",
    "        self.dense1 = Dense(self.s * self.s * self.n_f)\n",
    "        self.reshape1 = Reshape(target_shape = (self.s, self.s, self.n_f))\n",
    "        self.conv1 = Conv2DTranspose(512, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2DTranspose(256, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.conv3 = Conv2DTranspose(128, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.conv4 = Conv2DTranspose(64, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.conv5 = Conv2DTranspose(3, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs # inputs [batch, z_dim]\n",
    "        x = self.dense1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.reshape1(x)\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training = training))\n",
    "        # No BatchNormalization for output layer of Generator\n",
    "        x = tf.tanh(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.s = 4\n",
    "        self.k = 4\n",
    "        #self.n_f = 1024\n",
    "        \n",
    "        self.conv1 = Conv2D(64, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.conv2 = Conv2D(128, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.conv3 = Conv2D(256, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.conv4 = Conv2D(512, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs\n",
    "        x = tf.nn.leaky_relu(self.conv1(x))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training = training))\n",
    "        x = self.dense(self.flatten(x))\n",
    "       # no sigmoid? \n",
    "        return x\n",
    "        \n",
    "        # No BatchNormalization for input layer of Discriminator\n",
    "        \n",
    "def main():\n",
    "    generator = Generator()\n",
    "    generator.build(input_shape=(None, z_dim))\n",
    "    #generator.summary()\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "    #discriminator.summary()\n",
    "    \n",
    "    g_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    d_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    \n",
    "    d_losses, g_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(1):\n",
    "            batch_z = get_random_z(z_dim, batch_size)\n",
    "            batch_x = next(db_iter)\n",
    "            \n",
    "            with tf.GradientTape() as d_tape:\n",
    "                d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "            d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "        \n",
    "        batch_zz = get_random_z(z_dim, batch_size)\n",
    "        batch_xx = next(db_iter)\n",
    "        \n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_loss = g_loss_fn(generator, discriminator, batch_zz, is_training)\n",
    "        g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"D-loss:\", float(d_loss), \"G-loss:\", float(g_loss))\n",
    "            z = tf.random.uniform([100, z_dim], minval=-1, maxval=1)\n",
    "            generated_images = generator(z, training = False)\n",
    "            img_path = os.path.join('gan_images', 'gan-%d.png'%epoch)\n",
    "            save_result(generated_images.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "            d_losses.append(float(d_loss))\n",
    "            g_losses.append(float(g_loss))\n",
    "\n",
    "        if epoch % 10000 == 1:\n",
    "            generator.save_weights('generator.ckpt')\n",
    "            discriminator.save_weights('discriminator.ckpt')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "def build_discriminator():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must provide an `input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-37322414b467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0mgenerator_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m \u001b[0mgenerator_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;31m#generator_seq.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must provide an `input_shape` argument.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You must provide an `input_shape` argument."
     ]
    }
   ],
   "source": [
    "# DCGAN implementation with Sequential mode\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, ReLU, LeakyReLU, Activation\n",
    "from tensorflow.keras import Sequential, optimizers\n",
    "import  os,multiprocessing\n",
    "import  numpy as np\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import  glob\n",
    "\n",
    "def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n",
    "\n",
    "    # @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.image.resize(img, [resize, resize])\n",
    "        # img = tf.image.random_crop(img,[resize, resize])\n",
    "        # img = tf.image.random_flip_left_right(img)\n",
    "        # img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.clip_by_value(img, 0, 255)\n",
    "        img = img / 127.5 - 1 #-1~1\n",
    "        return img\n",
    "\n",
    "    dataset = disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) // batch_size\n",
    "\n",
    "    return dataset, img_shape, len_dataset\n",
    "\n",
    "\n",
    "def batch_dataset(dataset,\n",
    "                  batch_size,\n",
    "                  drop_remainder=True,\n",
    "                  n_prefetch_batch=1,\n",
    "                  filter_fn=None,\n",
    "                  map_fn=None,\n",
    "                  n_map_threads=None,\n",
    "                  filter_after_map=False,\n",
    "                  shuffle=True,\n",
    "                  shuffle_buffer_size=None,\n",
    "                  repeat=None):\n",
    "    # set defaults\n",
    "    if n_map_threads is None:\n",
    "        n_map_threads = multiprocessing.cpu_count()\n",
    "    if shuffle and shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
    "\n",
    "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if not filter_after_map:\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "    else:  # [*] this is slower\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def memory_data_batch_dataset(memory_data,\n",
    "                              batch_size,\n",
    "                              drop_remainder=True,\n",
    "                              n_prefetch_batch=1,\n",
    "                              filter_fn=None,\n",
    "                              map_fn=None,\n",
    "                              n_map_threads=None,\n",
    "                              filter_after_map=False,\n",
    "                              shuffle=True,\n",
    "                              shuffle_buffer_size=None,\n",
    "                              repeat=None):\n",
    "    \"\"\"Batch dataset of memory data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_data : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
    "    dataset = batch_dataset(dataset,\n",
    "                            batch_size,\n",
    "                            drop_remainder=drop_remainder,\n",
    "                            n_prefetch_batch=n_prefetch_batch,\n",
    "                            filter_fn=filter_fn,\n",
    "                            map_fn=map_fn,\n",
    "                            n_map_threads=n_map_threads,\n",
    "                            filter_after_map=filter_after_map,\n",
    "                            shuffle=shuffle,\n",
    "                            shuffle_buffer_size=shuffle_buffer_size,\n",
    "                            repeat=repeat)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def disk_image_batch_dataset(img_paths,\n",
    "                             batch_size,\n",
    "                             labels=None,\n",
    "                             drop_remainder=True,\n",
    "                             n_prefetch_batch=1,\n",
    "                             filter_fn=None,\n",
    "                             map_fn=None,\n",
    "                             n_map_threads=None,\n",
    "                             filter_after_map=False,\n",
    "                             shuffle=True,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             repeat=None):\n",
    "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_paths : 1d-tensor/ndarray/list of str\n",
    "        labels : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        memory_data = img_paths\n",
    "    else:\n",
    "        memory_data = (img_paths, labels)\n",
    "\n",
    "    def parse_fn(path, *label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n",
    "        return (img,) + label\n",
    "\n",
    "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
    "        def map_fn_(*args):\n",
    "            return map_fn(*parse_fn(*args))\n",
    "    else:\n",
    "        map_fn_ = parse_fn\n",
    "\n",
    "    dataset = memory_data_batch_dataset(memory_data,\n",
    "                                        batch_size,\n",
    "                                        drop_remainder=drop_remainder,\n",
    "                                        n_prefetch_batch=n_prefetch_batch,\n",
    "                                        filter_fn=filter_fn,\n",
    "                                        map_fn=map_fn_,\n",
    "                                        n_map_threads=n_map_threads,\n",
    "                                        filter_after_map=filter_after_map,\n",
    "                                        shuffle=shuffle,\n",
    "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                        repeat=repeat)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    #toimage(final_image).save(image_path)\n",
    "    #print(image_path)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    y = tf.ones_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    y = tf.zeros_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def g_loss_fn(g, d, batch_z, training):\n",
    "    logits = d(g(batch_z,training=training),training=training)\n",
    "    return celoss_ones(logits)\n",
    "    \n",
    "def d_loss_fn(g, d, batch_z, batch_x, training):\n",
    "    fake_logits = d(g(batch_z, training=training),training=training)\n",
    "    real_logits = d(batch_x, training=training)\n",
    "    return celoss_zeros(fake_logits) + celoss_ones(real_logits)\n",
    "\n",
    "layers_generator = [\n",
    "    Dense(8*8*1024),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    Reshape(target_shape=[8,8,1024]),\n",
    "    \n",
    "    # Layer 1\n",
    "    Conv2DTranspose(filters=512, kernel_size=4, strides=1, padding='same', use_bias=False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    # Layer 2\n",
    "    Conv2DTranspose(filters = 256, kernel_size = 4, strides = 2, padding = 'same', use_bias = False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    # Layer 3\n",
    "    Conv2DTranspose(filters = 128, kernel_size = 4, strides = 2, padding = 'same', use_bias = False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    # Layer 4\n",
    "    Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same',use_bias=False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    # Layer 5 (Output Layer)\n",
    "    Conv2DTranspose(filters=3, kernel_size=4, strides=1, padding='same', use_bias=False, activation=None),\n",
    "    Activation('tanh')\n",
    "]\n",
    "\n",
    "generator_seq = Sequential(layers_generator)\n",
    "generator_seq.build(input_shape=[None,z_dim])\n",
    "#generator_seq.summary()\n",
    "\n",
    "layers_discriminator = [\n",
    "    # Layer 1\n",
    "    Conv2D(filters=64, kernel_size=4, strides=2, padding='same', use_bias=False, activation=None),\n",
    "    LeakyReLU(0.2),\n",
    "    \n",
    "    # Layer 2\n",
    "    Conv2D(filters=128, kernel_size=4, strides=2, padding='same', use_bias=False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(0.2),\n",
    "    \n",
    "    # Layer 3\n",
    "    Conv2D(filters=256, kernel_size=4, strides=2, padding='same', use_bias=False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(0.2),\n",
    "   \n",
    "    # Layer 4\n",
    "    Conv2D(filters=512, kernel_size=4, strides=2, padding='same', use_bias=False, activation=None),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(0.2),\n",
    "   \n",
    "    # Layer 5\n",
    "    Conv2D(filters=1, kernel_size=4, strides=1, padding='valid', use_bias=False, activation=None),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "discriminator_seq = Sequential(layers_discriminator)\n",
    "discriminator_seq.build(input_shape=[None,64,64,3])\n",
    "#discriminator_seq.summary()\n",
    "\n",
    "g_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "d_optimizer = optimizers.Adam(learning_rate=0.00025, beta_1=0.5)\n",
    "\n",
    "# Test code for Sequential mode starts here\n",
    "z_dim = 100\n",
    "epochs = 100000\n",
    "batch_size = 64\n",
    "\n",
    "# Data Loading\n",
    "img_path = glob.glob('./faces/*.jpg')\n",
    "print('Num of files:',len(img_path))\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize=64)\n",
    "dataset = dataset.repeat(100)\n",
    "db_iter = iter(dataset)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset)) # 采样\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(),\n",
    "          tf.reduce_min(sample).numpy())\n",
    "\n",
    "d_losses, g_losses = [], []\n",
    "# for e in range(epochs):\n",
    "#     noise = tf.random.normal([batch_size, 100])\n",
    "#     batch_x = next(db_iter)\n",
    "#     \n",
    "#     with tf.GradientTape() as r_tape:\n",
    "#         real_logits = discriminator_seq(batch_x)\n",
    "#         d_loss_real = celoss_ones(real_logits)\n",
    "#         d_grad_real = r_tape.gradient(d_loss_real, discriminator_seq.trainable_variables)\n",
    "#         d_optimizer.apply_gradients(zip(d_grad_real, discriminator_seq.trainable_variables))\n",
    "#     \n",
    "#     with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "#         fake_logits = discriminator_seq(generator_seq(noise, training=True), training=True)\n",
    "#         d_loss_fake = celoss_zeros(fake_logits)\n",
    "#         g_loss = celoss_ones(fake_logits)\n",
    "#         d_grad_fake = d_tape.gradient(d_loss_fake, discriminator_seq.trainable_variables)\n",
    "#         g_grad = g_tape.gradient(g_loss, generator_seq.trainable_variables)\n",
    "#         \n",
    "#         d_optimizer.apply_gradients(zip(d_grad_fake, discriminator_seq.trainable_variables))\n",
    "#         g_optimizer.apply_gradients(zip(g_grad, generator_seq.trainable_variables))\n",
    "#         \n",
    "#     if e % 100 == 0:\n",
    "#         print(\"Epoch: \", e, \"D-loss:\", float(0.5*d_loss_real + 0.5*d_loss_fake), \"G-loss:\", float(g_loss))\n",
    "#         test_noise = tf.random.normal([100, z_dim])\n",
    "#         fake_image = generator_seq(test_noise, False)\n",
    "#         img_path = os.path.join('./gan_images', 'gan-%d.png'%e)\n",
    "#         save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "# \n",
    "#         d_losses.append(float(d_loss))\n",
    "#         g_losses.append(float(g_loss))\n",
    "# \n",
    "#         if e % 10000 == 1:\n",
    "#             # print(d_losses)\n",
    "#             # print(g_losses)\n",
    "#             generator.save_weights('generator.ckpt')\n",
    "#             discriminator.save_weights('discriminator.ckpt')\n",
    "for e in range(epochs):\n",
    "    for _ in range(1):\n",
    "        batch_z = tf.random.normal([batch_size, z_dim], mean=0, stddev=1)\n",
    "        batch_x = next(db_iter)\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            d_loss = d_loss_fn(generator_seq, discriminator_seq, batch_z, batch_x, True)\n",
    "        d_grads = d_tape.gradient(d_loss, discriminator_seq.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(d_grads, discriminator_seq.trainable_variables))\n",
    "    \n",
    "    batch_z = tf.random.normal([batch_size, z_dim], mean=0, stddev=1)\n",
    "    batch_x = next(db_iter)\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        g_loss = g_loss_fn(generator_seq, discriminator_seq, batch_z, True)\n",
    "    g_grads = g_tape.gradient(g_loss, generator_seq.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, generator_seq.trainable_variables))\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch: \", e, \"D-loss:\", float(d_loss), \"G-loss:\", float(g_loss))\n",
    "        test_noise = tf.random.normal([100, z_dim])\n",
    "        fake_image = generator_seq(test_noise, False)\n",
    "        img_path = os.path.join('./gan_images', 'gan-%d.png'%e)\n",
    "        save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "        d_losses.append(float(d_loss))\n",
    "        g_losses.append(float(g_loss))\n",
    "\n",
    "        if e % 10000 == 1:\n",
    "            # print(d_losses)\n",
    "            # print(g_losses)\n",
    "            generator.save_weights('generator.ckpt')\n",
    "            discriminator.save_weights('discriminator.ckpt')\n",
    "print(\"Done!\") \n",
    "# The result is close to perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN implementation with Functional API mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
