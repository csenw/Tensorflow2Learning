{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs models -- DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Libs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dev20200315\n"
     ]
    }
   ],
   "source": [
    "# All required libs are imporeted in this cell\n",
    "import os, glob, multiprocessing\n",
    "import tensorflow as tf\n",
    "import  numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, ReLU, LeakyReLU, Activation\n",
    "from tensorflow.keras import Sequential, optimizers, Input\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "z_dim = 100\n",
    "epochs = 3000000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "is_training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Required Functions/Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Files: 51223\n",
      "<PrefetchDataset shapes: (128, 64, 64, 3), types: tf.float32> (64, 64, 3)\n",
      "(128, 64, 64, 3) 1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n",
    "\n",
    "    # @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.image.resize(img, [resize, resize])\n",
    "        # img = tf.image.random_crop(img,[resize, resize])\n",
    "        # img = tf.image.random_flip_left_right(img)\n",
    "        # img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.clip_by_value(img, 0, 255)\n",
    "        img = img / 127.5 - 1 #-1~1\n",
    "        return img\n",
    "\n",
    "    dataset = disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) // batch_size\n",
    "\n",
    "    return dataset, img_shape, len_dataset\n",
    "\n",
    "\n",
    "def batch_dataset(dataset,\n",
    "                  batch_size,\n",
    "                  drop_remainder=True,\n",
    "                  n_prefetch_batch=1,\n",
    "                  filter_fn=None,\n",
    "                  map_fn=None,\n",
    "                  n_map_threads=None,\n",
    "                  filter_after_map=False,\n",
    "                  shuffle=True,\n",
    "                  shuffle_buffer_size=None,\n",
    "                  repeat=None):\n",
    "    # set defaults\n",
    "    if n_map_threads is None:\n",
    "        n_map_threads = multiprocessing.cpu_count()\n",
    "    if shuffle and shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
    "\n",
    "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if not filter_after_map:\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "    else:  # [*] this is slower\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def memory_data_batch_dataset(memory_data,\n",
    "                              batch_size,\n",
    "                              drop_remainder=True,\n",
    "                              n_prefetch_batch=1,\n",
    "                              filter_fn=None,\n",
    "                              map_fn=None,\n",
    "                              n_map_threads=None,\n",
    "                              filter_after_map=False,\n",
    "                              shuffle=True,\n",
    "                              shuffle_buffer_size=None,\n",
    "                              repeat=None):\n",
    "    \"\"\"Batch dataset of memory data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_data : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
    "    dataset = batch_dataset(dataset,\n",
    "                            batch_size,\n",
    "                            drop_remainder=drop_remainder,\n",
    "                            n_prefetch_batch=n_prefetch_batch,\n",
    "                            filter_fn=filter_fn,\n",
    "                            map_fn=map_fn,\n",
    "                            n_map_threads=n_map_threads,\n",
    "                            filter_after_map=filter_after_map,\n",
    "                            shuffle=shuffle,\n",
    "                            shuffle_buffer_size=shuffle_buffer_size,\n",
    "                            repeat=repeat)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def disk_image_batch_dataset(img_paths,\n",
    "                             batch_size,\n",
    "                             labels=None,\n",
    "                             drop_remainder=True,\n",
    "                             n_prefetch_batch=1,\n",
    "                             filter_fn=None,\n",
    "                             map_fn=None,\n",
    "                             n_map_threads=None,\n",
    "                             filter_after_map=False,\n",
    "                             shuffle=True,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             repeat=None):\n",
    "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_paths : 1d-tensor/ndarray/list of str\n",
    "        labels : nested structure of tensors/ndarrays/lists\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        memory_data = img_paths\n",
    "    else:\n",
    "        memory_data = (img_paths, labels)\n",
    "\n",
    "    def parse_fn(path, *label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n",
    "        return (img,) + label\n",
    "\n",
    "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
    "        def map_fn_(*args):\n",
    "            return map_fn(*parse_fn(*args))\n",
    "    else:\n",
    "        map_fn_ = parse_fn\n",
    "\n",
    "    dataset = memory_data_batch_dataset(memory_data,\n",
    "                                        batch_size,\n",
    "                                        drop_remainder=drop_remainder,\n",
    "                                        n_prefetch_batch=n_prefetch_batch,\n",
    "                                        filter_fn=filter_fn,\n",
    "                                        map_fn=map_fn_,\n",
    "                                        n_map_threads=n_map_threads,\n",
    "                                        filter_after_map=filter_after_map,\n",
    "                                        shuffle=shuffle,\n",
    "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                        repeat=repeat)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_random_z(z_dim, batch_size):\n",
    "    return tf.random.uniform([batch_size, z_dim], minval=-1, maxval=1)\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    y = tf.ones_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits = True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    y = tf.zeros_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y, logits, from_logits = True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Loss function for Discriminator\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    #print(fake_image)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    \n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    \n",
    "    return d_loss_fake + d_loss_real\n",
    "\n",
    "def d_loss_fn2(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    #print(fake_image)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    \n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    \n",
    "    return d_loss_fake, d_loss_real, 0.5*d_loss_fake + 0.5*d_loss_real\n",
    "\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "    fake_image = generator(batch_z, is_training)\n",
    "    d_fake_logits = discriminator(fake_image, is_training)\n",
    "    return celoss_ones(d_fake_logits)\n",
    "\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    #toimage(final_image).save(image_path)\n",
    "    #print(image_path)\n",
    "    Image.fromarray(final_image).save(image_path)\n",
    "    \n",
    "img_path = glob.glob('./faces/*.jpg')\n",
    "print(\"Num of Files:\",len(img_path))\n",
    "\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize = 64)\n",
    "print(dataset, img_shape)\n",
    "sample = next(iter(dataset))\n",
    "print(sample.shape, tf.reduce_max(sample).numpy(), tf.reduce_min(sample).numpy())\n",
    "dataset = dataset.repeat(100)\n",
    "db_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definintions and Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubClass Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 32, 64)\n",
      "(None, 16, 16, 128)\n",
      "(None, 8, 8, 256)\n",
      "(None, 4, 4, 512)\n",
      "(None, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "Epoch: 0 D-loss: 1.4889330863952637 G-loss: 6.032578468322754\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n",
      "(128, 32, 32, 64)\n",
      "(128, 16, 16, 128)\n",
      "(128, 8, 8, 256)\n",
      "(128, 4, 4, 512)\n",
      "(128, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e54a9f74c51d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discriminator.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-e54a9f74c51d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_zz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mg_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m           data_format=op.get_attr(\"data_format\").decode())\n\u001b[0m\u001b[1;32m     64\u001b[0m   ]\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m   r\"\"\"Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.s = 2\n",
    "        self.k = 4\n",
    "        self.n_f = 1024\n",
    "        \n",
    "        self.dense1 = Dense(self.s * self.s * self.n_f)\n",
    "        self.reshape1 = Reshape(target_shape = (self.s, self.s, self.n_f))\n",
    "        self.conv1 = Conv2DTranspose(512, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2DTranspose(256, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.conv3 = Conv2DTranspose(128, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.conv4 = Conv2DTranspose(64, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.conv5 = Conv2DTranspose(3, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs # inputs [batch, z_dim]\n",
    "        x = self.dense1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.reshape1(x)\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training = training))\n",
    "        # No BatchNormalization for output layer of Generator\n",
    "        x = tf.tanh(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.s = 4\n",
    "        self.k = 4\n",
    "        #self.n_f = 1024\n",
    "        \n",
    "        self.conv1 = Conv2D(64, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.conv2 = Conv2D(128, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.conv3 = Conv2D(256, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.conv4 = Conv2D(512, kernel_size = self.k, strides = 2, padding = 'same')\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        x = inputs\n",
    "        x = tf.nn.leaky_relu(self.conv1(x))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training = training))\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training = training))\n",
    "        x = self.dense(self.flatten(x))\n",
    "        return x\n",
    "        \n",
    "        # No BatchNormalization for input layer of Discriminator\n",
    "        \n",
    "def main():\n",
    "    generator = Generator()\n",
    "    generator.build(input_shape=(None, z_dim))\n",
    "    #generator.summary()\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    discriminator.build(input_shape=(None, 64, 64, 3))\n",
    "    #discriminator.summary()\n",
    "    \n",
    "    g_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    d_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    \n",
    "    d_losses, g_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(1):\n",
    "            batch_z = get_random_z(z_dim, batch_size)\n",
    "            batch_x = next(db_iter)\n",
    "            \n",
    "            with tf.GradientTape() as d_tape:\n",
    "                d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "            d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "        \n",
    "        batch_zz = get_random_z(z_dim, batch_size)\n",
    "        batch_xx = next(db_iter)\n",
    "        \n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_loss = g_loss_fn(generator, discriminator, batch_zz, is_training)\n",
    "        g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"D-loss:\", float(d_loss), \"G-loss:\", float(g_loss))\n",
    "            z = tf.random.uniform([100, z_dim], minval=-1, maxval=1)\n",
    "            generated_images = generator(z, training = False)\n",
    "            img_path = os.path.join('gan_images', 'gan-%d.png'%epoch)\n",
    "            save_result(generated_images.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "            d_losses.append(float(d_loss))\n",
    "            g_losses.append(float(g_loss))\n",
    "\n",
    "        if epoch % 10000 == 1:\n",
    "            generator.save_weights('generator.ckpt')\n",
    "            discriminator.save_weights('discriminator.ckpt')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 D-loss: 1.5948282480239868 G-loss: 5.737722396850586\n",
      "Epoch: 100 D-loss: 1.0108578205108643 G-loss: 2.7868332862854004\n",
      "Epoch: 200 D-loss: 1.5291911363601685 G-loss: 4.22914981842041\n",
      "Epoch: 300 D-loss: 0.8474067449569702 G-loss: 3.167870044708252\n",
      "Epoch: 400 D-loss: 0.9078989624977112 G-loss: 5.156543254852295\n",
      "Epoch: 500 D-loss: 1.0400261878967285 G-loss: 2.2575838565826416\n",
      "Epoch: 600 D-loss: 0.8453784584999084 G-loss: 4.009437084197998\n",
      "Epoch: 700 D-loss: 0.6565330028533936 G-loss: 3.2830617427825928\n",
      "Epoch: 800 D-loss: 0.8159675002098083 G-loss: 4.475857734680176\n",
      "Epoch: 900 D-loss: 0.5756043195724487 G-loss: 2.435945510864258\n",
      "Epoch: 1000 D-loss: 1.4895097017288208 G-loss: 2.402553081512451\n",
      "Epoch: 1100 D-loss: 0.6468749642372131 G-loss: 3.6792359352111816\n",
      "Epoch: 1200 D-loss: 1.1983184814453125 G-loss: 3.726593255996704\n",
      "Epoch: 1300 D-loss: 0.9159888625144958 G-loss: 4.501795768737793\n",
      "Epoch: 1400 D-loss: 1.3910218477249146 G-loss: 1.8285226821899414\n",
      "Epoch: 1500 D-loss: 0.9081695079803467 G-loss: 3.0908641815185547\n",
      "Epoch: 1600 D-loss: 0.692634105682373 G-loss: 3.40633487701416\n",
      "Epoch: 1700 D-loss: 0.5405380129814148 G-loss: 2.5803475379943848\n",
      "Epoch: 1800 D-loss: 0.9773578643798828 G-loss: 2.2493767738342285\n",
      "Epoch: 1900 D-loss: 0.8674769997596741 G-loss: 5.243636131286621\n",
      "Epoch: 2000 D-loss: 0.9184747338294983 G-loss: 2.4429495334625244\n",
      "Epoch: 2100 D-loss: 0.6001982688903809 G-loss: 3.0119848251342773\n",
      "Epoch: 2200 D-loss: 0.8226711750030518 G-loss: 5.346708297729492\n",
      "Epoch: 2300 D-loss: 0.4768812954425812 G-loss: 3.8137917518615723\n",
      "Epoch: 2400 D-loss: 1.615878939628601 G-loss: 8.002784729003906\n",
      "Epoch: 2500 D-loss: 0.6369751691818237 G-loss: 3.98941707611084\n",
      "Epoch: 2600 D-loss: 0.6068497896194458 G-loss: 4.675341606140137\n",
      "Epoch: 2700 D-loss: 0.6062989830970764 G-loss: 3.644199848175049\n",
      "Epoch: 2800 D-loss: 0.46814876794815063 G-loss: 4.157351016998291\n",
      "Epoch: 2900 D-loss: 0.7356343865394592 G-loss: 1.9236421585083008\n",
      "Epoch: 3000 D-loss: 1.1861176490783691 G-loss: 6.7566328048706055\n",
      "Epoch: 3100 D-loss: 0.521336555480957 G-loss: 3.243126153945923\n",
      "Epoch: 3200 D-loss: 0.37830618023872375 G-loss: 4.644525051116943\n",
      "Epoch: 3300 D-loss: 0.5231867432594299 G-loss: 4.968608856201172\n",
      "Epoch: 3400 D-loss: 0.667717456817627 G-loss: 2.888676166534424\n",
      "Epoch: 3500 D-loss: 0.6626256108283997 G-loss: 6.653758525848389\n",
      "Epoch: 3600 D-loss: 1.3216193914413452 G-loss: 4.38191556930542\n",
      "Epoch: 3700 D-loss: 0.3808608055114746 G-loss: 5.320314407348633\n",
      "Epoch: 3800 D-loss: 0.795170247554779 G-loss: 9.48859977722168\n",
      "Epoch: 3900 D-loss: 0.27489739656448364 G-loss: 3.13214373588562\n",
      "Epoch: 4000 D-loss: 0.3680614233016968 G-loss: 6.966381072998047\n",
      "Epoch: 4100 D-loss: 0.24627457559108734 G-loss: 3.5361647605895996\n",
      "Epoch: 4200 D-loss: 0.5214802622795105 G-loss: 2.0537776947021484\n",
      "Epoch: 4300 D-loss: 0.20389944314956665 G-loss: 3.753818988800049\n",
      "Epoch: 4400 D-loss: 0.532214879989624 G-loss: 3.149712085723877\n",
      "Epoch: 4500 D-loss: 0.17794454097747803 G-loss: 4.401922225952148\n",
      "Epoch: 4600 D-loss: 0.3994361162185669 G-loss: 4.777243614196777\n",
      "Epoch: 4700 D-loss: 0.4324474632740021 G-loss: 3.7009036540985107\n",
      "Epoch: 4800 D-loss: 0.4632290005683899 G-loss: 7.123134613037109\n",
      "Epoch: 4900 D-loss: 0.252943217754364 G-loss: 4.412722587585449\n",
      "Epoch: 5000 D-loss: 0.2587622404098511 G-loss: 5.665047645568848\n",
      "Epoch: 5100 D-loss: 0.3719259798526764 G-loss: 3.7296836376190186\n",
      "Epoch: 5200 D-loss: 0.4041825234889984 G-loss: 4.11474084854126\n",
      "Epoch: 5300 D-loss: 0.27133938670158386 G-loss: 3.918008327484131\n",
      "Epoch: 5400 D-loss: 0.4268404245376587 G-loss: 4.4120941162109375\n",
      "Epoch: 5500 D-loss: 0.12713190913200378 G-loss: 3.025020122528076\n",
      "Epoch: 5600 D-loss: 0.3246394991874695 G-loss: 4.616116523742676\n",
      "Epoch: 5700 D-loss: 0.22443968057632446 G-loss: 4.17020320892334\n",
      "Epoch: 5800 D-loss: 0.22822155058383942 G-loss: 8.231547355651855\n",
      "Epoch: 5900 D-loss: 0.22277170419692993 G-loss: 3.549119234085083\n",
      "Epoch: 6000 D-loss: 0.22856570780277252 G-loss: 3.401674270629883\n",
      "Epoch: 6100 D-loss: 0.2086006999015808 G-loss: 3.7402100563049316\n",
      "Epoch: 6200 D-loss: 0.1509656012058258 G-loss: 5.273865699768066\n",
      "Epoch: 6300 D-loss: 0.4598116874694824 G-loss: 8.562761306762695\n",
      "Epoch: 6400 D-loss: 0.39838165044784546 G-loss: 8.377866744995117\n",
      "Epoch: 6500 D-loss: 1.9488106966018677 G-loss: 12.339614868164062\n",
      "Epoch: 6600 D-loss: 0.22116950154304504 G-loss: 4.40460205078125\n",
      "Epoch: 6700 D-loss: 0.7699199914932251 G-loss: 12.876884460449219\n",
      "Epoch: 6800 D-loss: 0.5872387886047363 G-loss: 6.047492980957031\n",
      "Epoch: 6900 D-loss: 0.2018897533416748 G-loss: 6.282487392425537\n",
      "Epoch: 7000 D-loss: 0.36904823780059814 G-loss: 9.776086807250977\n",
      "Epoch: 7100 D-loss: 0.2209469974040985 G-loss: 6.570241451263428\n",
      "Epoch: 7200 D-loss: 0.3075534701347351 G-loss: 8.519346237182617\n",
      "Epoch: 7300 D-loss: 0.14499321579933167 G-loss: 6.427487373352051\n",
      "Epoch: 7400 D-loss: 0.1917705535888672 G-loss: 5.768258094787598\n",
      "Epoch: 7500 D-loss: 0.08983093500137329 G-loss: 5.0276641845703125\n",
      "Epoch: 7600 D-loss: 0.42075955867767334 G-loss: 8.885886192321777\n",
      "Epoch: 7700 D-loss: 0.13594090938568115 G-loss: 4.639558792114258\n",
      "Epoch: 7800 D-loss: 0.2684122920036316 G-loss: 7.17302131652832\n",
      "Epoch: 7900 D-loss: 0.22074374556541443 G-loss: 7.774576187133789\n",
      "Epoch: 8000 D-loss: 0.03835337236523628 G-loss: 7.7396650314331055\n",
      "Epoch: 8100 D-loss: 0.203302264213562 G-loss: 7.089197635650635\n",
      "Epoch: 8200 D-loss: 0.22902342677116394 G-loss: 3.250673770904541\n",
      "Epoch: 8300 D-loss: 0.23691938817501068 G-loss: 6.986482620239258\n",
      "Epoch: 8400 D-loss: 0.1620176136493683 G-loss: 5.338191986083984\n",
      "Epoch: 8500 D-loss: 0.13412213325500488 G-loss: 4.453713417053223\n",
      "Epoch: 8600 D-loss: 0.18191425502300262 G-loss: 7.988461494445801\n",
      "Epoch: 8700 D-loss: 0.3254750072956085 G-loss: 5.090150833129883\n",
      "Epoch: 8800 D-loss: 0.5314692854881287 G-loss: 12.676673889160156\n",
      "Epoch: 8900 D-loss: 0.3763154447078705 G-loss: 5.608259201049805\n",
      "Epoch: 9000 D-loss: 0.15573696792125702 G-loss: 7.329197883605957\n",
      "Epoch: 9100 D-loss: 0.1525663286447525 G-loss: 8.449788093566895\n",
      "Epoch: 9200 D-loss: 0.1395474523305893 G-loss: 4.497542381286621\n",
      "Epoch: 9300 D-loss: 0.16031885147094727 G-loss: 4.019370079040527\n",
      "Epoch: 9400 D-loss: 1.0221589803695679 G-loss: 6.150031089782715\n",
      "Epoch: 9500 D-loss: 0.13025906682014465 G-loss: 4.339645862579346\n",
      "Epoch: 9600 D-loss: 0.17080914974212646 G-loss: 5.47243595123291\n",
      "Epoch: 9700 D-loss: 0.4671356976032257 G-loss: 3.2080068588256836\n",
      "Epoch: 9800 D-loss: 0.17183423042297363 G-loss: 8.620811462402344\n",
      "Epoch: 9900 D-loss: 0.3342672884464264 G-loss: 8.713770866394043\n",
      "Epoch: 10000 D-loss: 0.11853855848312378 G-loss: 5.590648174285889\n",
      "Epoch: 10100 D-loss: 0.33091986179351807 G-loss: 6.885298728942871\n",
      "Epoch: 10200 D-loss: 0.1683548092842102 G-loss: 7.661571979522705\n",
      "Epoch: 10300 D-loss: 1.3536704778671265 G-loss: 19.044727325439453\n",
      "Epoch: 10400 D-loss: 0.2526448667049408 G-loss: 6.814006805419922\n",
      "Epoch: 10500 D-loss: 0.17746582627296448 G-loss: 6.690325736999512\n",
      "Epoch: 10600 D-loss: 0.1093955710530281 G-loss: 5.093674659729004\n",
      "Epoch: 10700 D-loss: 0.538177490234375 G-loss: 14.89551830291748\n",
      "Epoch: 10800 D-loss: 0.1377057433128357 G-loss: 5.312580585479736\n",
      "Epoch: 10900 D-loss: 7.357728004455566 G-loss: 8.406620979309082\n",
      "Epoch: 11000 D-loss: 0.08800151944160461 G-loss: 5.794897079467773\n",
      "Epoch: 11100 D-loss: 0.10163192451000214 G-loss: 5.618490219116211\n",
      "Epoch: 11200 D-loss: 0.13913434743881226 G-loss: 5.77720308303833\n",
      "Epoch: 11300 D-loss: 0.14125174283981323 G-loss: 5.784456253051758\n",
      "Epoch: 11400 D-loss: 0.12076959758996964 G-loss: 8.48344612121582\n",
      "Epoch: 11500 D-loss: 0.2209087610244751 G-loss: 5.2210693359375\n",
      "Epoch: 11600 D-loss: 0.4726633131504059 G-loss: 9.168155670166016\n",
      "Epoch: 11700 D-loss: 0.09842196106910706 G-loss: 5.077025413513184\n",
      "Epoch: 11800 D-loss: 0.1894039660692215 G-loss: 5.818958759307861\n",
      "Epoch: 11900 D-loss: 0.11171369254589081 G-loss: 4.990274429321289\n",
      "Epoch: 12000 D-loss: 0.9864382147789001 G-loss: 1.8976025581359863\n",
      "Epoch: 12100 D-loss: 0.1188574880361557 G-loss: 7.219867706298828\n",
      "Epoch: 12200 D-loss: 0.680861234664917 G-loss: 6.412931442260742\n",
      "Epoch: 12300 D-loss: 0.11360275000333786 G-loss: 3.9434280395507812\n",
      "Epoch: 12400 D-loss: 0.08049958944320679 G-loss: 4.845716953277588\n",
      "Epoch: 12500 D-loss: 0.10349346697330475 G-loss: 6.187981605529785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12600 D-loss: 0.15633945167064667 G-loss: 7.504440784454346\n",
      "Epoch: 12700 D-loss: 0.049245819449424744 G-loss: 8.571148872375488\n",
      "Epoch: 12800 D-loss: 0.33248794078826904 G-loss: 5.765406608581543\n",
      "Epoch: 12900 D-loss: 0.018209179863333702 G-loss: 7.256743431091309\n",
      "Epoch: 13000 D-loss: 0.09191304445266724 G-loss: 4.788175582885742\n",
      "Epoch: 13100 D-loss: 0.10258734971284866 G-loss: 6.195735931396484\n",
      "Epoch: 13200 D-loss: 0.14156274497509003 G-loss: 5.407291889190674\n",
      "Epoch: 13300 D-loss: 0.21264532208442688 G-loss: 7.441789150238037\n",
      "Epoch: 13400 D-loss: 0.19592316448688507 G-loss: 8.862916946411133\n",
      "Epoch: 13500 D-loss: 0.17427287995815277 G-loss: 8.341344833374023\n",
      "Epoch: 13600 D-loss: 0.1856236755847931 G-loss: 5.131638526916504\n",
      "Epoch: 13700 D-loss: 0.41536784172058105 G-loss: 12.447162628173828\n",
      "Epoch: 13800 D-loss: 0.04603038728237152 G-loss: 4.747023582458496\n",
      "Epoch: 13900 D-loss: 0.11698566377162933 G-loss: 6.324587821960449\n",
      "Epoch: 14000 D-loss: 3.7645351886749268 G-loss: 8.499452590942383\n",
      "Epoch: 14100 D-loss: 0.06142313778400421 G-loss: 4.866223335266113\n",
      "Epoch: 14200 D-loss: 0.11344599723815918 G-loss: 5.16683292388916\n",
      "Epoch: 14300 D-loss: 0.08258187770843506 G-loss: 6.303921699523926\n",
      "Epoch: 14400 D-loss: 0.08799311518669128 G-loss: 6.296859264373779\n",
      "Epoch: 14500 D-loss: 0.08520413190126419 G-loss: 4.9337639808654785\n",
      "Epoch: 14600 D-loss: 0.04584911838173866 G-loss: 5.647690773010254\n",
      "Epoch: 14700 D-loss: 0.04832077398896217 G-loss: 5.297680854797363\n",
      "Epoch: 14800 D-loss: 0.1156240850687027 G-loss: 7.041931629180908\n",
      "Epoch: 14900 D-loss: 0.1022416278719902 G-loss: 18.07923126220703\n",
      "Epoch: 15000 D-loss: 0.07342952489852905 G-loss: 4.964529037475586\n",
      "Epoch: 15100 D-loss: 0.3867712914943695 G-loss: 5.452931880950928\n",
      "Epoch: 15200 D-loss: 0.1003468930721283 G-loss: 6.2442097663879395\n",
      "Epoch: 15300 D-loss: 0.07900281995534897 G-loss: 5.728840351104736\n",
      "Epoch: 15400 D-loss: 0.039343006908893585 G-loss: 6.884443283081055\n",
      "Epoch: 15500 D-loss: 0.06749277561903 G-loss: 4.282547950744629\n",
      "Epoch: 15600 D-loss: 0.09114368259906769 G-loss: 6.529818534851074\n",
      "Epoch: 15700 D-loss: 0.09696648269891739 G-loss: 6.165691375732422\n",
      "Epoch: 15800 D-loss: 0.130804643034935 G-loss: 5.960207462310791\n",
      "Epoch: 15900 D-loss: 0.07838417589664459 G-loss: 5.063628196716309\n",
      "Epoch: 16000 D-loss: 0.11742500960826874 G-loss: 6.058145046234131\n",
      "Epoch: 16100 D-loss: 0.04813122749328613 G-loss: 5.787022590637207\n",
      "Epoch: 16200 D-loss: 0.10406000167131424 G-loss: 4.3064141273498535\n",
      "Epoch: 16300 D-loss: 0.05360034480690956 G-loss: 7.539980888366699\n",
      "Epoch: 16400 D-loss: 0.07127305865287781 G-loss: 11.030678749084473\n",
      "Epoch: 16500 D-loss: 0.09447421878576279 G-loss: 8.646934509277344\n",
      "Epoch: 16600 D-loss: 0.06376063078641891 G-loss: 5.9744553565979\n",
      "Epoch: 16700 D-loss: 0.10104511678218842 G-loss: 5.543532371520996\n",
      "Epoch: 16800 D-loss: 0.05179941654205322 G-loss: 6.399702072143555\n",
      "Epoch: 16900 D-loss: 0.03846876695752144 G-loss: 4.8463897705078125\n",
      "Epoch: 17000 D-loss: 1.4138559103012085 G-loss: 3.278078556060791\n",
      "Epoch: 17100 D-loss: 0.13924163579940796 G-loss: 6.6031880378723145\n",
      "Epoch: 17200 D-loss: 0.1770186722278595 G-loss: 6.601611137390137\n",
      "Epoch: 17300 D-loss: 0.10837084800004959 G-loss: 5.102298259735107\n",
      "Epoch: 17400 D-loss: 0.09108004719018936 G-loss: 9.202417373657227\n",
      "Epoch: 17500 D-loss: 0.0997108519077301 G-loss: 7.140284538269043\n",
      "Epoch: 17600 D-loss: 0.12753260135650635 G-loss: 7.264374732971191\n",
      "Epoch: 17700 D-loss: 0.07768049836158752 G-loss: 7.039485931396484\n",
      "Epoch: 17800 D-loss: 0.1589932143688202 G-loss: 6.326825141906738\n",
      "Epoch: 17900 D-loss: 0.1023353859782219 G-loss: 6.047204971313477\n",
      "Epoch: 18000 D-loss: 0.1068761944770813 G-loss: 17.317703247070312\n",
      "Epoch: 18100 D-loss: 2.326298236846924 G-loss: 2.19024920463562\n",
      "Epoch: 18200 D-loss: 0.25277259945869446 G-loss: 18.179534912109375\n",
      "Epoch: 18300 D-loss: 0.11606810986995697 G-loss: 7.932584285736084\n",
      "Epoch: 18400 D-loss: 0.45081910490989685 G-loss: 8.812446594238281\n",
      "Epoch: 18500 D-loss: 0.039336927235126495 G-loss: 8.688993453979492\n",
      "Epoch: 18600 D-loss: 0.19220159947872162 G-loss: 5.474254608154297\n",
      "Epoch: 18700 D-loss: 0.07142405211925507 G-loss: 6.094898223876953\n",
      "Epoch: 18800 D-loss: 0.071151003241539 G-loss: 5.761145114898682\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5dfa924ae9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdis_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discriminator.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-5dfa924ae9e1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mbatch_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Subclass needs to specify input when calling .build(None,...)\n",
    "# API needs to specify input shape without batch dim.\n",
    "def build_generator_model(z_dim = 100, batch_size = 128, s = 2 , k = 4 , n_filters = 1024):    \n",
    "    noise = Input(shape = [z_dim])\n",
    "    x = Dense(s*s*n_filters)(noise)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape(target_shape=[s, s, n_filters])(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Conv 1\n",
    "    x = Conv2DTranspose(filters = 512, kernel_size = k, strides = 2, padding ='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Conv 2\n",
    "    x = Conv2DTranspose(filters = 256, kernel_size = k, strides = 2, padding ='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Conv 3\n",
    "    x = Conv2DTranspose(filters = 128, kernel_size = k, strides = 2, padding ='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Conv 4\n",
    "    x = Conv2DTranspose(filters = 64, kernel_size = k, strides = 2, padding ='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Conv 5\n",
    "    x = Conv2DTranspose(filters = 3, kernel_size = k, strides = 2, padding = 'same')(x)\n",
    "    logits = Activation('tanh', name='logits')(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    return tf.keras.Model(noise, logits, name='Generator')\n",
    "\n",
    "def build_discriminator_model(z_dim=100, k = 4):\n",
    "    image_input = Input(shape=[64, 64, 3])\n",
    "    x = Conv2D(filters = 64, kernel_size = k, strides = 2, padding = 'same')(image_input)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    x = Conv2D(filters = 128, kernel_size = k, strides = 2, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    x = Conv2D(filters = 256, kernel_size = k, strides = 2, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    x = Conv2D(filters = 512, kernel_size = k, strides = 2, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    logits = Dense(1)(x)\n",
    "    #print(logits.shape)\n",
    "    return tf.keras.Model(image_input, logits, name='Discriminator')\n",
    "\n",
    "def main():\n",
    "    gen_model = build_generator_model()\n",
    "    dis_model = build_discriminator_model()\n",
    "    \n",
    "    g_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    d_opt = optimizers.Adam(learning_rate = learning_rate, beta_1=0.5)\n",
    "    \n",
    "    d_losses, g_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(1):\n",
    "            batch_z = get_random_z(z_dim, batch_size)\n",
    "            batch_x = next(db_iter)\n",
    "            \n",
    "            with tf.GradientTape() as d_tape:\n",
    "                d_loss = d_loss_fn(gen_model, dis_model, batch_z, batch_x, is_training)\n",
    "            d_grads = d_tape.gradient(d_loss, dis_model.trainable_variables)\n",
    "            d_opt.apply_gradients(zip(d_grads, dis_model.trainable_variables))\n",
    "        \n",
    "        batch_zz = get_random_z(z_dim, batch_size)\n",
    "        batch_xx = next(db_iter)\n",
    "\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_loss = g_loss_fn(gen_model, dis_model, batch_zz, is_training)\n",
    "        g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n",
    "        g_opt.apply_gradients(zip(g_grads, gen_model.trainable_variables))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"D-loss:\", float(d_loss), \"G-loss:\", float(g_loss))\n",
    "            z = tf.random.uniform([100, z_dim], minval=-1, maxval=1)\n",
    "            generated_images = gen_model(z, training = False)\n",
    "            img_path = os.path.join('gan_images', 'api_gan-%d.png'%epoch)\n",
    "            save_result(generated_images.numpy(), 10, img_path, color_mode='P')\n",
    "\n",
    "            d_losses.append(float(d_loss))\n",
    "            g_losses.append(float(g_loss))\n",
    "\n",
    "        if epoch % 10000 == 1:\n",
    "            gen_model.save_weights('generator.ckpt')\n",
    "            dis_model.save_weights('discriminator.ckpt')\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN implementation with Functional API mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
