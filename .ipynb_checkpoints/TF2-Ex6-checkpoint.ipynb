{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GoogLeNet (Inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:  caltech101 Number of Classes:  102\n",
      "3060\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Squeeze: Dst tensor is not initialized. [Op:Squeeze]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2d90315f70c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtest_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtest_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2d90315f70c8>\u001b[0m in \u001b[0;36mgetDb\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1513\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[1;32m   4143\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4144\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4145\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   9863\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9864\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9865\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9866\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9867\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Squeeze: Dst tensor is not initialized. [Op:Squeeze]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, optimizers, Input, Model, losses\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dropout, Dense, Flatten, LayerNormalization, concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_datasets as tfds\n",
    "import pydot\n",
    "\n",
    "dataset_name = 'caltech101'\n",
    "ds_info = tfds.builder(dataset_name).info\n",
    "# 101 classes and 1 background class\n",
    "num_classes = ds_info.features['label'].num_classes\n",
    "num_epochs = 100\n",
    "num_batch_sizes = 64\n",
    "print(\"Datasets: \", dataset_name, \"Number of Classes: \", num_classes)\n",
    "train_ds, test_ds = tfds.load(dataset_name, split=['train','test'])\n",
    "\n",
    "def preprocess(x,y):\n",
    "    x = 2*tf.cast(x, dtype = tf.float32)/255. -1\n",
    "    y = tf.cast(y, dtype = tf.int32)\n",
    "    return x, y\n",
    "\n",
    "def getDb(ds):\n",
    "    x, y = [], []\n",
    "    for sample in ds:\n",
    "        x.append(tf.image.resize(sample[\"image\"], size = [224, 224]))\n",
    "        y.append(sample[\"label\"])\n",
    "    db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    print(len(x))\n",
    "    return db\n",
    "\n",
    "train_db = getDb(train_ds)\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(num_batch_sizes)\n",
    "test_db = getDb(test_ds)\n",
    "test_db = test_db.map(preprocess).batch(num_batch_sizes)\n",
    "\n",
    "print(\"Data sets constructed!\")\n",
    "print(\"Num of Classes:\", num_classes)\n",
    "#print(train_db, test_db)\n",
    "\n",
    "def getInceptionModule(x, filters_11, filters_113, filters_33, filters_115, filters_55, filters_11mp):\n",
    "    x_11 = Conv2D(filters = filters_11, kernel_size = [1, 1], strides = 1, activation='relu')(x)\n",
    "    \n",
    "    x_113 = Conv2D(filters = filters_113, kernel_size = [1, 1], strides = 1, activation='relu')(x)\n",
    "    x_33 = Conv2D(filters = filters_33, kernel_size = [3, 3], strides = 1, padding = 'same', activation = 'relu')(x_113)\n",
    "    \n",
    "    x_115 = Conv2D(filters = filters_115, kernel_size = [1, 1], strides = 1, activation='relu')(x)\n",
    "    x_55 = Conv2D(filters = filters_55, kernel_size = [5, 5], strides = 1, padding='same', activation='relu')(x_115)\n",
    "    \n",
    "    x_mp = MaxPool2D(pool_size = [3, 3], padding = 'same', strides = 1)(x)\n",
    "    x_mp11 = Conv2D(filters = filters_11mp, kernel_size = [1,1], strides = 1, activation='relu')(x_mp)\n",
    "    \n",
    "    return concatenate([x_11, x_33, x_55, x_mp11]) \n",
    "\n",
    "def getGoogleNet():\n",
    "    googlenet_inputs = Input(shape=[224, 224, 3])\n",
    "    x = Conv2D(64, kernel_size = [7,7], strides = 2, padding = 'same', activation='relu')(googlenet_inputs)\n",
    "    x = MaxPool2D(pool_size=[3,3], strides = 2, padding = 'same')(x)\n",
    "    #x = LayerNormalization(epsilon = 1e-6)(x)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=[1,1], activation = 'relu')(x)\n",
    "    x = Conv2D(192, kernel_size=[3,3], padding='same', activation = 'relu')(x)\n",
    "    x = MaxPool2D(pool_size=[3,3], strides = 2, padding = 'same')(x)\n",
    "    #x = LayerNormalization(epsilon = 1e-6)(x)\n",
    "\n",
    "    inc_3a = getInceptionModule(x, 64, 96, 128, 16, 32, 32)\n",
    "    inc_3b = getInceptionModule(inc_3a, 128, 128, 192, 32, 96, 64)\n",
    "    x = MaxPool2D(pool_size = [3, 3], strides = 2, padding = 'same')(inc_3b)\n",
    "\n",
    "    inc_4a = getInceptionModule(x, 192, 96, 208, 16, 48, 64)\n",
    "    inc_4b = getInceptionModule(inc_4a, 160, 112, 224, 24, 64, 64)\n",
    "    inc_4c = getInceptionModule(inc_4b, 128, 128, 256, 24, 64, 64)\n",
    "    inc_4d = getInceptionModule(inc_4c, 112, 144, 288, 32, 64, 64)\n",
    "    inc_4e = getInceptionModule(inc_4d, 256, 160, 320, 32, 128, 128)\n",
    "    x = MaxPool2D(pool_size = [3, 3], strides = 2, padding = 'same')(inc_4e)\n",
    "\n",
    "    inc_5a = getInceptionModule(x, 256, 160, 320, 32, 128, 128)\n",
    "    inc_5b = getInceptionModule(inc_5a, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    avg_pool_6 = AveragePooling2D(pool_size = [7, 7], padding = 'same')(inc_5b)\n",
    "    x = Flatten()(avg_pool_6)\n",
    "    x = Dropout(rate=0.4)(x)\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    outputs_7 = Dense(num_classes, activation=None)(x)\n",
    "\n",
    "    outputs_4a = AveragePooling2D(pool_size= [5, 5], strides = [3,3], padding = 'valid')(inc_4a)\n",
    "    outputs_4a = Conv2D(filters = 128, kernel_size = [1,1], strides = [1,1], padding = 'same', activation='relu')(outputs_4a)\n",
    "    outputs_4a = Flatten()(outputs_4a)\n",
    "    outputs_4a = Dense(1024, activation='relu')(outputs_4a)\n",
    "    outputs_4a = Dropout(0.7)(outputs_4a)\n",
    "    outputs_4a = Dense(num_classes, activation='softmax')(outputs_4a)\n",
    "\n",
    "    outputs_4d = AveragePooling2D(pool_size= [5, 5], strides = [3,3], padding = 'valid')(inc_4d)\n",
    "    outputs_4d = Conv2D(filters = 128, kernel_size = [1,1], strides = [1,1], padding = 'same', activation='relu')(outputs_4d)\n",
    "    outputs_4d = Flatten()(outputs_4d)\n",
    "    outputs_4d = Dense(1024, activation='relu')(outputs_4d)\n",
    "    outputs_4d = Dropout(0.7)(outputs_4d)\n",
    "    outputs_4d = Dense(num_classes, activation='softmax')(outputs_4d)\n",
    "    #googlenet_model = Model(inputs = googlenet_inputs, outputs = [outputs_7, outputs_4a, outputs_4d])\n",
    "    googlenet_outputs = [outputs_7, outputs_4a, outputs_4d]\n",
    "    print(googlenet_outputs)\n",
    "    googlenet_model = Model(inputs = googlenet_inputs, outputs = googlenet_outputs)\n",
    "    \n",
    "    return googlenet_model\n",
    "\n",
    "googlenet = getGoogleNet()\n",
    "googlenet.compile(optimizer = optimizers.SGD(learning_rate=0.0001, momentum=0.9),\n",
    "                  loss = losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                  metrics = ['accuracy'])\n",
    "#googlenet.summary()\n",
    "#plot_model(googlenet, to_file='googlenet_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "history = googlenet.fit(train_db, batch_size = num_batch_sizes, epochs = num_epochs)\n",
    "test_scores = googlenet.evaluate(test_db, verbose=2)\n",
    "\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
